{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:43.628313Z",
     "start_time": "2019-07-21T03:27:34.819524Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "import sys\n",
    "from collections import Counter \n",
    "import pprint \n",
    "import math\n",
    "import argparse \n",
    "import os\n",
    "import collections\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "import time \n",
    "import pandas as pd\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "seed_value= 0\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.set_random_seed(seed_value)\n",
    "from keras import backend as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=4, inter_op_parallelism_threads=4)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:46.300541Z",
     "start_time": "2019-07-21T03:27:43.631478Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_json(filepath):\n",
    "    \"\"\"\n",
    "    function used to parse json of each commit json file\n",
    "\n",
    "    Args:\n",
    "        filepath_list - list of filepaths\n",
    "\n",
    "    Returns:\n",
    "        files_json - list object contains parsed information\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    files_json = []\n",
    "    commit_ids = []\n",
    "    # each commits\n",
    "    files = os.listdir(filepath)\n",
    "    for path in files:\n",
    "        commit_id = path.split(\"_\")[1].split(\".\")[0]\n",
    "        if os.stat(filepath + path).st_size != 0 and path != 'desktop.ini':\n",
    "            with open(filepath + path, encoding=\"utf8\") as f:\n",
    "                data = json.load(f)\n",
    "                files_list = []\n",
    "                # each file in commits\n",
    "                for file in data['files']:\n",
    "                    # parse only cluster file\n",
    "                    for key in file.keys():\n",
    "                        if re.match('^.*_cluster$', key):\n",
    "                            actions_list = []\n",
    "                            actions = file[key]['actions']\n",
    "                            # each action in file\n",
    "                            for action in actions:\n",
    "                                actions_list.append(action['root'])\n",
    "                            files_list.append(actions_list)\n",
    "            if len(files_list) != 0:\n",
    "                files_json.append(files_list)\n",
    "                commit_ids.append(commit_id)\n",
    "    assert(len(commit_ids) == len(files_json))      \n",
    "    # return\n",
    "    return files_json, commit_ids\n",
    "\n",
    "\n",
    "folder_path = './tmp_JSON_labeled_commits/'\n",
    "all_files, csha = parse_json(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:46.324762Z",
     "start_time": "2019-07-21T03:27:46.305599Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_roots(files_data):\n",
    "    counting = {}\n",
    "    for file_index, files in enumerate(files_data):\n",
    "        for root_index, roots in enumerate(files):\n",
    "            for action_index, actions in enumerate(roots):\n",
    "                temp = actions.split(' at ')[0].strip()\n",
    "                tempq = []\n",
    "                if temp.startswith('INS'):\n",
    "                    tempq.append('INS')\n",
    "                    words = [temp.split('INS ')[1].split('to ')[0].strip()] + [\n",
    "                        temp.split('INS ')[1].rsplit('to ')[-1].strip()\n",
    "                    ]\n",
    "                    for items in words:\n",
    "                        items = items.split(':')[0].strip()\n",
    "                        tempq.append(items)\n",
    "                    if tempq[1] == 'TextElement' and tempq[-1] not in ['TagElement', 'TextElement']:\n",
    "                        tempq[-1] = ''\n",
    "                    temp = '_'.join(tempq)\n",
    "\n",
    "                if temp.startswith('UPDATE'):\n",
    "                    temp = 'UPDATE'\n",
    "                if temp.startswith('MOVE'):\n",
    "                    temp2 = temp.split('from ')[1].strip()\n",
    "                    tempq.append('MOVE')\n",
    "                    tempq.append(temp2.split(':')[0].strip())\n",
    "                    temp = '_'.join(tempq)\n",
    "\n",
    "                if temp.startswith('DEL'):\n",
    "                    tempq.append('DEL')\n",
    "                    tempq.append(temp.split('DEL ')[1].split(':')[0].strip())\n",
    "                    temp = '_'.join(tempq)\n",
    "                temp = temp.replace(' ', '_')\n",
    "                counting[temp] = counting.get(temp, 0) + 1\n",
    "                files_data[file_index][root_index][action_index] = temp\n",
    "    dic = {}\n",
    "    i = 0\n",
    "    for k, v in counting.items():\n",
    "        dic[k] = i\n",
    "        i += 1\n",
    "    return dic, files_data, counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:46.475539Z",
     "start_time": "2019-07-21T03:27:46.328083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n"
     ]
    }
   ],
   "source": [
    "dic, datas, freq_dict = preprocess_roots(all_files)\n",
    "rev_dic = dict(zip(dic.values(), dic.keys()))\n",
    "print(len(dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:46.488683Z",
     "start_time": "2019-07-21T03:27:46.478368Z"
    }
   },
   "outputs": [],
   "source": [
    "def actions2sentence(datas):\n",
    "    data_total = []\n",
    "    for files in datas:\n",
    "        data4file = []\n",
    "        for roots in files:\n",
    "            sentence = ' '.join(roots)\n",
    "            data4file.append(sentence)\n",
    "        data_total.append(data4file)\n",
    "    return data_total\n",
    "\n",
    "\n",
    "training_data = actions2sentence(datas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation: \n",
    "Prepare data for embedding and training ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.handle_labels import drop_labels\n",
    "from utils.handle_labels import group_labels\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutate_files(csha, training_data): \n",
    "    commits_dic = dict()\n",
    "    for sha, training_file in zip(csha, training_data): \n",
    "        commits_dic[sha] = []\n",
    "        if len(training_file) <= 1: \n",
    "            tmp_permutate = list(itertools.permutations(training_file))\n",
    "            for permutated_file in tmp_permutate: \n",
    "                commits_dic[sha].append(list(permutated_file))\n",
    "        else: \n",
    "            commits_dic[sha].append(training_file)\n",
    "    return commits_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_list(commits_labels_df):\n",
    "    s= commits_labels_df.apply(lambda x: pd.Series(x['Files']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "    s.name = \"Files\"\n",
    "    commits_labels_df = commits_labels_df.drop(\"Files\", axis=1) \n",
    "    commits_labels_df = commits_labels_df.join(s)\n",
    "    return commits_labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1922, 28)\n",
      "<class 'list'>\n",
      "['Testing', 'Bug fix']\n",
      "                                          0  \\\n",
      "0  3dd2210e79a8eb84378c370b32652f9a53f87a93   \n",
      "1  e691b66aadbed87ac4891cec2ca5136bc85cfe4d   \n",
      "2  1f959d076ed7f29c3f8a5c6e99cbfcc62c1058d9   \n",
      "3  1a7286afce71c005bae8d45e6b280e977f823a79   \n",
      "4  4c0a65457cb7a16578592cfb2278a2bb99f78cad   \n",
      "\n",
      "                                                   1  \n",
      "0  [[INS_ImportDeclaration_CompilationUnit INS_Fi...  \n",
      "1  [[INS_VariableDeclarationStatement_Block INS_T...  \n",
      "2  [[INS_ImportDeclaration_CompilationUnit INS_Im...  \n",
      "3  [[INS_ImportDeclaration_CompilationUnit INS_Im...  \n",
      "4  [[MOVE_SingleVariableDeclaration DEL_TypeParam...  \n",
      "exp_train_df shape: (624, 13)\n",
      "train_df shape: (624, 13)\n",
      "test_df shape: (153, 13)\n"
     ]
    }
   ],
   "source": [
    "# merge csha and training data to a dataframe\n",
    "commits_df = pd.DataFrame(data = [csha, training_data]).T\n",
    "commits_df.columns = [\"Commit ID\", \"Files\"]\n",
    "df_new = pd.read_csv('./data/commit_data_new.csv')\n",
    "print(df_new.shape)\n",
    "\n",
    "# import new dataset(contains 2000 commits) with labels\n",
    "# convert string to list\n",
    "df_new['categories'] = df_new['categories'].apply(lambda x: literal_eval(x))\n",
    "print(type(df_new['categories'].values[0]))\n",
    "print(df_new['categories'].values[0])\n",
    "df_new = df_new.drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "# merge two dataframe and drop some of labels\n",
    "commits_labels_df = pd.merge(commits_df, df_new, on='Commit ID')\n",
    "commits_labels_df.head(1)\n",
    "commits_labels_df = drop_labels(commits_labels_df, [\"Testing\", \"Build\", \"Versioning\", \"Indentation\", \"Internationalization\", \"Merge\", \\\n",
    "                                                   \"Module Move\", \"Module Remove\", \"Source Control\", \"Rename\", \"Initialization\", \\\n",
    "                                                   \"Module Add\", \"Data\"])\n",
    "commits_labels_df = group_labels(commits_labels_df, [\"Cross\", \"Debug\"], \"Cross_\")\n",
    "commits_labels_df = group_labels(commits_labels_df, [\"Legal\", \"Documentation\"], \"Documentation_\")\n",
    "commits_labels_df.shape\n",
    "\n",
    "# split dataframe to train and test\n",
    "msk = np.random.rand(len(commits_labels_df)) < 0.8\n",
    "train_df = commits_labels_df[msk]\n",
    "test_df = commits_labels_df[~msk]\n",
    "\n",
    "# permutate train_df\n",
    "permutate_train_dic = permutate_files(train_df['Commit ID'],train_df['Files'])\n",
    "permutate_train_df = pd.DataFrame(list(permutate_train_dic.items()))\n",
    "print(permutate_train_df.head())\n",
    "permutate_train_df.columns = ['Commit ID','Files']\n",
    "train_df = train_df.drop([\"Files\"], axis=1)\n",
    "train_df['Files'] = permutate_train_df['Files'].values\n",
    "\n",
    "# expanded train_df list\n",
    "expanded_train_df = expand_list(train_df)\n",
    "\n",
    "print('exp_train_df shape:',expanded_train_df.shape)\n",
    "print('train_df shape:',train_df.shape)\n",
    "print('test_df shape:',test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:46.502749Z",
     "start_time": "2019-07-21T03:27:46.491433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INS_ImportDeclaration_CompilationUnit INS_FieldDeclaration_TypeDeclaration INS_FieldDeclaration_TypeDeclaration UPDATE MOVE_VariableDeclarationFragment INS_ExpressionStatement_Block INS_ExpressionStatement_Block UPDATE UPDATE INS_SimpleName_MethodInvocation INS_SimpleName_MethodInvocation INS_SimpleName_MethodInvocation INS_ClassInstanceCreation_MethodInvocation DEL_SimpleName DEL_ExpressionStatement DEL_SimpleName INS_ImportDeclaration_CompilationUnit INS_VariableDeclarationStatement_Block INS_IfStatement_Block INS_ImportDeclaration_CompilationUnit INS_FieldDeclaration_TypeDeclaration INS_SingleVariableDeclaration_MethodDeclaration INS_SingleVariableDeclaration_MethodDeclaration INS_ExpressionStatement_Block INS_ExpressionStatement_Block INS_ExpressionStatement_Block INS_ExpressionStatement_Block INS_ExpressionStatement_Block INS_ExpressionStatement_Block UPDATE INS_ExpressionStatement_Block INS_IfStatement_Block INS_ExpressionStatement_Block INS_ExpressionStatement_Block UPDATE INS_MethodInvocation_MethodInvocation INS_MethodInvocation_MethodInvocation INS_ImportDeclaration_CompilationUnit INS_VariableDeclarationStatement_Block INS_IfStatement_Block INS_ImportDeclaration_CompilationUnit INS_ImportDeclaration_CompilationUnit INS_MethodDeclaration_TypeDeclaration INS_MethodDeclaration_TypeDeclaration INS_IfStatement_Block INS_MethodInvocation_MethodInvocation INS_ImportDeclaration_CompilationUnit INS_MethodDeclaration_TypeDeclaration INS_MethodDeclaration_TypeDeclaration INS_MethodDeclaration_TypeDeclaration INS_FieldDeclaration_TypeDeclaration INS_MethodDeclaration_TypeDeclaration INS_MethodDeclaration_TypeDeclaration MOVE_MethodDeclaration MOVE_MethodDeclaration INS_SingleVariableDeclaration_MethodDeclaration INS_Block_MethodDeclaration MOVE_MethodDeclaration INS_SingleVariableDeclaration_MethodDeclaration INS_SingleVariableDeclaration_MethodDeclaration MOVE_Block INS_ExpressionStatement_Block INS_MethodInvocation_ReturnStatement UPDATE MOVE_SimpleType UPDATE MOVE_SimpleType INS_SimpleName_SuperConstructorInvocation INS_SimpleName_SuperConstructorInvocation UPDATE MOVE_MethodInvocation MOVE_MethodInvocation INS_SimpleName_ClassInstanceCreation INS_SimpleName_ClassInstanceCreation DEL_ClassInstanceCreation DEL_MethodDeclaration DEL_MethodDeclaration\n",
      "  \n",
      "INS_Block_MethodDeclaration MOVE_Block DEL_FieldDeclaration DEL_ConstructorInvocation DEL_MethodDeclaration DEL_Block UPDATE UPDATE UPDATE\n"
     ]
    }
   ],
   "source": [
    "def concat_files_to_sentence(expanded_train_list): \n",
    "    concat_data = \"\"\n",
    "    tmp_list = []\n",
    "    for items in expanded_train_list:\n",
    "        concat_data = \" \".join(items)\n",
    "        tmp_list.append(concat_data)\n",
    "    return tmp_list\n",
    "concat_train_data = concat_files_to_sentence(expanded_train_df[\"Files\"])\n",
    "concat_test_data = concat_files_to_sentence(test_df[\"Files\"])\n",
    "print(concat_train_data[0])\n",
    "print(\"  \")\n",
    "print(concat_test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine File Threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.112668Z",
     "start_time": "2019-07-21T03:27:46.510367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sample training data>:  ['INS_ImportDeclaration_CompilationUnit INS_FieldDeclaration_TypeDeclaration INS_FieldDeclaration_TypeDeclaration UPDATE MOVE_VariableDeclarationFragment INS_ExpressionStatement_Block INS_ExpressionStatement_Block UPDATE UPDATE INS_SimpleName_MethodInvocation INS_SimpleName_MethodInvocation INS_SimpleName_MethodInvocation INS_ClassInstanceCreation_MethodInvocation DEL_SimpleName DEL_ExpressionStatement DEL_SimpleName', 'INS_ImportDeclaration_CompilationUnit INS_VariableDeclarationStatement_Block INS_IfStatement_Block', 'INS_ImportDeclaration_CompilationUnit INS_FieldDeclaration_TypeDeclaration INS_SingleVariableDeclaration_MethodDeclaration INS_SingleVariableDeclaration_MethodDeclaration INS_ExpressionStatement_Block INS_ExpressionStatement_Block INS_ExpressionStatement_Block INS_ExpressionStatement_Block INS_ExpressionStatement_Block INS_ExpressionStatement_Block UPDATE INS_ExpressionStatement_Block INS_IfStatement_Block INS_ExpressionStatement_Block INS_ExpressionStatement_Block UPDATE INS_MethodInvocation_MethodInvocation INS_MethodInvocation_MethodInvocation', 'INS_ImportDeclaration_CompilationUnit INS_VariableDeclarationStatement_Block INS_IfStatement_Block', 'INS_ImportDeclaration_CompilationUnit INS_ImportDeclaration_CompilationUnit INS_MethodDeclaration_TypeDeclaration INS_MethodDeclaration_TypeDeclaration INS_IfStatement_Block INS_MethodInvocation_MethodInvocation', 'INS_ImportDeclaration_CompilationUnit INS_MethodDeclaration_TypeDeclaration INS_MethodDeclaration_TypeDeclaration INS_MethodDeclaration_TypeDeclaration INS_FieldDeclaration_TypeDeclaration INS_MethodDeclaration_TypeDeclaration INS_MethodDeclaration_TypeDeclaration MOVE_MethodDeclaration MOVE_MethodDeclaration INS_SingleVariableDeclaration_MethodDeclaration INS_Block_MethodDeclaration MOVE_MethodDeclaration INS_SingleVariableDeclaration_MethodDeclaration INS_SingleVariableDeclaration_MethodDeclaration MOVE_Block INS_ExpressionStatement_Block INS_MethodInvocation_ReturnStatement UPDATE MOVE_SimpleType UPDATE MOVE_SimpleType INS_SimpleName_SuperConstructorInvocation INS_SimpleName_SuperConstructorInvocation UPDATE MOVE_MethodInvocation MOVE_MethodInvocation INS_SimpleName_ClassInstanceCreation INS_SimpleName_ClassInstanceCreation DEL_ClassInstanceCreation DEL_MethodDeclaration DEL_MethodDeclaration']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_seqlength(training_data):\n",
    "    max_root_len = 0\n",
    "    seqlength_list = []\n",
    "    for item in training_data:\n",
    "        seqlength_list.append(len(item.split()))\n",
    "        if len(item.split()) >  max_root_len: \n",
    "            max_root_len = len(item.split())\n",
    "    return max_root_len, seqlength_list\n",
    "\n",
    "def plot_hist(seqlength_list): \n",
    "    plt.figure(figsize=(20,10))\n",
    "    number_of_files = np.array(seqlength_list)\n",
    "    bincount = np.bincount(seqlength_list)\n",
    "    x = np.arange(1, len(bincount)+1)\n",
    "    n, bins, patches = plt.hist(seqlength_list,x)\n",
    "    plt.xlim((0, 500))\n",
    "    plt.ylim((0, 200))\n",
    "\n",
    "max_seqlength, sequence_list = get_seqlength(concat_train_data)\n",
    "print(\"<sample training data>: \", training_data[0])\n",
    "plot_hist(sequence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.136546Z",
     "start_time": "2019-07-21T03:27:53.117407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241\n"
     ]
    }
   ],
   "source": [
    "# getting file threshold\n",
    "threshold = 0.95\n",
    "number_of_actions = [len(item.split()) for item in concat_train_data]\n",
    "\n",
    "def get_file_threshold(number_of_files, threshold = 0.95):\n",
    "    '''\n",
    "    get padding threshold for files dimension\n",
    "    \n",
    "    Args:\n",
    "        number_of_files - array of the number of files in each commits\n",
    "        threshold - drop all commits with its the number of files beyond this threshold\n",
    "    Returns:\n",
    "        padding threshold - number\n",
    "    '''\n",
    "    \n",
    "    total_files = len(number_of_files)\n",
    "    number_of_files = np.array(number_of_files)\n",
    "    bincount = np.bincount(number_of_files)\n",
    "\n",
    "    sum_file = 0\n",
    "    for index, item in enumerate(bincount):\n",
    "        sum_file += item\n",
    "        #print(index,item)\n",
    "        #print(sum_file)\n",
    "        if sum_file > threshold*total_files:\n",
    "            padding_files_threshold = index\n",
    "            break\n",
    "            \n",
    "    return padding_files_threshold\n",
    "\n",
    "length_threshold = get_file_threshold(number_of_actions, threshold)\n",
    "print(length_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_train_df = expanded_train_df.drop([\"Files\"], axis=1)\n",
    "test_df = test_df.drop([\"Files\"], axis=1)\n",
    "expanded_train_df[\"Files\"] = concat_train_data \n",
    "test_df[\"Files\"] = concat_test_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.530703Z",
     "start_time": "2019-07-21T03:27:53.490766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Commit ID', 'project name', 'commit_message', 'Maintenance',\n",
      "       'Feature Add', 'Bug fix', 'Clean up', 'Refactoring', 'Token Replace',\n",
      "       'categories', 'Cross_', 'Documentation_', 'Files', 'len_seq'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "expanded_train_df['len_seq'] = expanded_train_df.apply(lambda row: len(row['Files'].split()), axis = 1)\n",
    "test_df['len_seq'] = test_df.apply(lambda row: len(row['Files'].split()), axis = 1)\n",
    "expanded_train_df = expanded_train_df[expanded_train_df['len_seq'] <= length_threshold].reset_index(drop = True)\n",
    "test_df = test_df[test_df['len_seq'] <= length_threshold].reset_index(drop = True)\n",
    "print(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.960319Z",
     "start_time": "2019-07-21T03:27:47.213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traing labels shape:  (593,)\n",
      "test labels shape:  (143,)\n"
     ]
    }
   ],
   "source": [
    "target_col = [\"Maintenance\", \"Feature Add\", \"Bug fix\", \"Clean up\", \"Refactoring\", \"Token Replace\", \"Cross_\", \"Documentation_\"]\n",
    "y_train = expanded_train_df[\"Maintenance\"].values\n",
    "y_test = test_df[\"Maintenance\"].values\n",
    "print(\"traing labels shape: \", y_train.shape) \n",
    "print(\"test labels shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Pad data \n",
    "We tokenize the data and pad with the token <PAD/>.<br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.966249Z",
     "start_time": "2019-07-21T03:27:47.781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 11, 11, 1, 35, 6, 6, 1, 1, 10, 10, 10, 85, 7, 15, 7, 2, 12, 17, 2, 11, 18, 18, 6, 6, 6, 6, 6, 6, 1, 6, 17, 6, 6, 1, 27, 27, 2, 12, 17, 2, 2, 3, 3, 17, 27, 2, 3, 3, 3, 11, 3, 3, 16, 16, 18, 37, 16, 18, 18, 8, 6, 76, 1, 19, 1, 19, 194, 194, 1, 5, 5, 48, 48, 58, 13, 13]\n",
      "[38, 6, 28, 164, 15, 21, 1, 1, 1]\n",
      "(593, 242)\n",
      "(143, 242)\n"
     ]
    }
   ],
   "source": [
    "#Training \n",
    "train_docs = expanded_train_df['Files'].values\n",
    "t_train = Tokenizer(filters = '', lower=False)\n",
    "t_train.fit_on_texts(train_docs)\n",
    "\n",
    "#Testing \n",
    "test_docs = test_df['Files'].values \n",
    "t_test = Tokenizer(filters = '', lower=False)\n",
    "t_test.fit_on_texts(test_docs)\n",
    "\n",
    "sequences_train = t_train.texts_to_sequences(train_docs)\n",
    "sequences_test = t_test.texts_to_sequences(test_docs)\n",
    "print(sequences_train[0])\n",
    "print(sequences_test[0])\n",
    "\n",
    "#Pad training data \n",
    "padded_seq_train = pad_sequences(sequences_train, maxlen=length_threshold + 1, padding=\"post\", truncating=\"post\")\n",
    "print(padded_seq_train.shape)\n",
    "\n",
    "#Pad testing data \n",
    "padded_seq_test = pad_sequences(sequences_test, maxlen=length_threshold + 1, padding=\"post\", truncating=\"post\")\n",
    "print(padded_seq_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Testing and Training Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.968217Z",
     "start_time": "2019-07-21T03:27:48.097Z"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary_train = t_train.word_index\n",
    "vocabulary_test = t_test.word_index \n",
    "\n",
    "\n",
    "vocabulary_inv_train = dict((v, k) for k, v in vocabulary_train.items())\n",
    "vocabulary_inv_test = dict((v, k) for k, v in vocabulary_test.items())\n",
    "vocabulary_inv_train[0] = \"<PAD/>\"\n",
    "vocabulary_inv_test[0] = \"<PAD/>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.969406Z",
     "start_time": "2019-07-21T03:27:48.398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(593, 242)\n",
      "(143, 242)\n",
      "(593,)\n",
      "(143,)\n"
     ]
    }
   ],
   "source": [
    "X_train = padded_seq_train \n",
    "X_test = padded_seq_test\n",
    "print(X_train[10, :])\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(filename):\n",
    "    \"\"\"\n",
    "    load embedding as python dictionary {root<str>: embeddings<np_array>}\n",
    "    :param filename: embedding.txt \n",
    "    :return: dictionary object mapping root to embeddings \n",
    "    \"\"\"\n",
    "    if not os.path.exists(filename): \n",
    "        print(\"please run 'Store Pre-Trained Embeddings Cell!'\")\n",
    "    else: \n",
    "        with open(filename, \"r\") as f: \n",
    "            lines = f.readlines()\n",
    "            f.close()\n",
    "            # create map of words to vectors \n",
    "            embedding = dict()\n",
    "            for line in lines: \n",
    "                comp = line.split()\n",
    "                # map of <str, numpy array> \n",
    "                embedding[comp[0]] = np.asarray(comp[1:], dtype='float32')\n",
    "            return embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_embed = load_embedding(\"embedding.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.979720Z",
     "start_time": "2019-07-21T03:27:50.237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train static shape: (593, 242, 300)\n",
      "x_test static shape: (143, 242, 300)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.stack([np.stack([pre_embed[vocabulary_inv_train[action]] for action in commit]) for commit in X_train])\n",
    "X_test = np.stack([np.stack([pre_embed[vocabulary_inv_test[action]] for action in commit]) for commit in X_test])\n",
    "print(\"x_train static shape:\", X_train.shape)\n",
    "print(\"x_test static shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN for text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.982096Z",
     "start_time": "2019-07-21T03:27:50.555Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras import regularizers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.992898Z",
     "start_time": "2019-07-21T03:27:51.761Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def print_evaluation_scores(y_test, predicted):\n",
    "    \n",
    "    print('Accuracy:', accuracy_score(y_test, predicted))\n",
    "    print('F1-score macro:', f1_score(y_test, predicted, average='macro'))\n",
    "    print('F1-score micro:', f1_score(y_test, predicted, average='micro'))\n",
    "    print('F1-score weighted:', f1_score(y_test, predicted, average='weighted'))\n",
    "    print('Hamming_loss:', hamming_loss(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.987127Z",
     "start_time": "2019-07-21T03:27:50.921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 593 samples, validate on 143 samples\n",
      "Epoch 1/20\n",
      " - 6s - loss: 0.7021 - acc: 0.4671 - val_loss: 0.6720 - val_acc: 0.5594\n",
      "Epoch 2/20\n",
      " - 2s - loss: 0.6582 - acc: 0.5683 - val_loss: 0.6705 - val_acc: 0.5245\n",
      "Epoch 3/20\n",
      " - 2s - loss: 0.6435 - acc: 0.5987 - val_loss: 0.6725 - val_acc: 0.5594\n",
      "Epoch 4/20\n",
      " - 3s - loss: 0.6223 - acc: 0.6341 - val_loss: 0.6729 - val_acc: 0.5734\n",
      "Epoch 5/20\n",
      " - 2s - loss: 0.6070 - acc: 0.6762 - val_loss: 0.6750 - val_acc: 0.5734\n",
      "Epoch 6/20\n",
      " - 2s - loss: 0.5923 - acc: 0.7066 - val_loss: 0.6777 - val_acc: 0.5524\n",
      "Epoch 7/20\n",
      " - 2s - loss: 0.5714 - acc: 0.7285 - val_loss: 0.6796 - val_acc: 0.5455\n",
      "Epoch 8/20\n",
      " - 2s - loss: 0.5564 - acc: 0.7184 - val_loss: 0.6817 - val_acc: 0.5524\n",
      "Epoch 9/20\n",
      " - 2s - loss: 0.5516 - acc: 0.7015 - val_loss: 0.6844 - val_acc: 0.5385\n",
      "Epoch 10/20\n",
      " - 2s - loss: 0.5306 - acc: 0.7622 - val_loss: 0.6890 - val_acc: 0.5594\n",
      "Epoch 11/20\n",
      " - 3s - loss: 0.5148 - acc: 0.7555 - val_loss: 0.6933 - val_acc: 0.5455\n",
      "Epoch 12/20\n",
      " - 3s - loss: 0.5046 - acc: 0.7555 - val_loss: 0.6983 - val_acc: 0.5455\n",
      "Epoch 13/20\n",
      " - 2s - loss: 0.4879 - acc: 0.7875 - val_loss: 0.7036 - val_acc: 0.5385\n",
      "Epoch 14/20\n",
      " - 2s - loss: 0.4764 - acc: 0.7926 - val_loss: 0.7103 - val_acc: 0.5455\n",
      "Epoch 15/20\n",
      " - 2s - loss: 0.4804 - acc: 0.7690 - val_loss: 0.7136 - val_acc: 0.5524\n",
      "Epoch 16/20\n",
      " - 2s - loss: 0.4694 - acc: 0.7757 - val_loss: 0.7154 - val_acc: 0.5385\n",
      "Epoch 17/20\n",
      " - 2s - loss: 0.4604 - acc: 0.7723 - val_loss: 0.7164 - val_acc: 0.5455\n",
      "Epoch 18/20\n",
      " - 2s - loss: 0.4430 - acc: 0.7875 - val_loss: 0.7181 - val_acc: 0.5455\n",
      "Epoch 19/20\n",
      " - 2s - loss: 0.4207 - acc: 0.8179 - val_loss: 0.7228 - val_acc: 0.5664\n",
      "Epoch 20/20\n",
      " - 2s - loss: 0.4222 - acc: 0.8094 - val_loss: 0.7290 - val_acc: 0.5804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x132d4f8d0>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyparameters\n",
    "model_type = \"CNN-non-static\"  # CNN-rand|CNN-non-static|CNN-static\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 300\n",
    "filter_sizes = (3,4,5)\n",
    "num_filters = 5\n",
    "dropout_prob = (0.5, 0.5)\n",
    "hidden_dims = 64\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 20 #50\n",
    "\n",
    "sequence_length = length_threshold\n",
    "\n",
    "# input\n",
    "input_shape = (sequence_length + 1, embedding_dim)\n",
    "model_input = Input(shape=input_shape)\n",
    "\n",
    "z = model_input\n",
    "\n",
    "# dropout layer\n",
    "# z = Dropout(dropout_prob[0])(z)\n",
    "\n",
    "# Convolutional block\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes: # Feature > Maintenance > Clean  up > Bug fix > \n",
    "    conv = Convolution1D(filters=num_filters,\n",
    "                         kernel_size=sz,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1)(z)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "\n",
    "z = Dropout(dropout_prob[1])(z)\n",
    "\n",
    "#z = Dense(hidden_dims, activation=\"relu\")(z)\n",
    "\n",
    "model_output = Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer= optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, \n",
    "                                                                     epsilon=None, decay=0.0, amsgrad=False), \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(X_test, y_test), verbose=2, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.994119Z",
     "start_time": "2019-07-21T03:27:52.337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5804195804195804\n",
      "F1-score macro: 0.5799059929494712\n",
      "F1-score micro: 0.5804195804195804\n",
      "F1-score weighted: 0.5812413203717551\n",
      "Hamming_loss: 0.4195804195804196\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred_bool = (y_pred > 0.5)\n",
    "\n",
    "predictions = y_pred_bool.astype(int)\n",
    "print_evaluation_scores(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           (None, 242, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 240, 5)       4505        input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 239, 5)       6005        input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 238, 5)       7505        input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 120, 5)       0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 119, 5)       0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 119, 5)       0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 600)          0           max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 595)          0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 595)          0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1790)         0           flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "                                                                 flatten_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 1790)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 1)            1791        dropout_27[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 19,806\n",
      "Trainable params: 19,806\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN for Text - Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(CNN_Text, self).__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        V = args.embed_num        # number of embedding\n",
    "        D = args.embed_dim        # embedding dimension\n",
    "        C = args.class_num        # number of class\n",
    "        \n",
    "        Ci = 1                    # input channel - number of channels of input data             \n",
    "        Co = args.kernel_num      # output channels - number of filters\n",
    "        Ks = args.kernel_sizes    # cnn kernel sizes - List - size dimension (conv_size, embedding_dimension)\n",
    "\n",
    "        #self.embed = nn.Embedding(V, D)                                       # embedding layer\n",
    "        # self.convs1 = [nn.Conv2d(Ci, Co, (K, D)) for K in Ks]\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])  # List of convolution layer\n",
    "        '''\n",
    "        self.conv13 = nn.Conv2d(Ci, Co, (3, D))\n",
    "        self.conv14 = nn.Conv2d(Ci, Co, (4, D))\n",
    "        self.conv15 = nn.Conv2d(Ci, Co, (5, D))\n",
    "        '''\n",
    "        self.dropout = nn.Dropout(args.dropout)                               # dropout layer\n",
    "        self.fc1 = nn.Linear(len(Ks)*Co, C)                                   # Dense Layer (input dimension, C classes)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3)  # (N, Co, W)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.embed(x)  # (N, W, D)\n",
    "        \n",
    "#         if self.args.static:\n",
    "#             x = Variable(x)\n",
    "\n",
    "        x = x.unsqueeze(1)  # (N, Ci, W, D)\n",
    "\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
    "\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
    "\n",
    "        x = torch.cat(x, 1)\n",
    "\n",
    "        '''\n",
    "        x1 = self.conv_and_pool(x,self.conv13) #(N,Co)\n",
    "        x2 = self.conv_and_pool(x,self.conv14) #(N,Co)\n",
    "        x3 = self.conv_and_pool(x,self.conv15) #(N,Co)\n",
    "        x = torch.cat((x1, x2, x3), 1) # (N,len(Ks)*Co)\n",
    "        '''\n",
    "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
    "        logit = self.fc1(x)  # (N, C)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeChangeDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, y, transform = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: dataframe contains features and labels\n",
    "            target_col : target columns name\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.X[idx]\n",
    "        label = np.asarray(self.y[idx])\n",
    "        sample = {'feature': sentence, 'label': label}\n",
    "\n",
    "        return sample\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        sentence, lable = sample['sentence'], sample['label']\n",
    "        \n",
    "        return {'feature': torch.from_numpy(sentence),\n",
    "                'label': torch.from_numpy(label)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(dataloader, model, args):\n",
    "    model.eval()\n",
    "    corrects, avg_loss = 0, 0\n",
    "    for idx, batch in enumerate(dataloader,0):\n",
    "        feature, target = batch['feature'], batch['label']\n",
    "        if args.cuda:\n",
    "            feature, target = feature.cuda(), target.cuda()\n",
    "\n",
    "        logit = model(feature)\n",
    "        logit = logit.squeeze(1)\n",
    "        loss = F.cross_entropy(logit, target, size_average=False)\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "        corrects += (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    avg_loss /= size\n",
    "    accuracy = 100.0 * corrects/size\n",
    "    print('Evaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss, accuracy, corrects, size))\n",
    "    return accuracy\n",
    "\n",
    "def train(dataloader, val_dataloader,  model, args):\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    \n",
    "    steps = 0\n",
    "    best_acc = 0\n",
    "    last_step = 0\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        running_loss = 0.0\n",
    "        for i, batch_data in enumerate(dataloader, 0):\n",
    "             # get the feature and target tensor\n",
    "            feature, target = batch['feature'], batch['label']\n",
    "            if args.cuda:\n",
    "                feature, target = feature.cuda(), target.cuda()\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # output : model(input)\n",
    "            output = model(feature)\n",
    "\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            steps += 1\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        # print training loss - each epoch\n",
    "        # corrects = (torch.max(output, 1)[1].view(target.size()).data == target.data).sum()\n",
    "        # accuracy = 100.0 * corrects/dataloader.batch_size\n",
    "        print('Epoch[{}] - loss: {:.6f}'.format(epoch, running_loss/i))\n",
    "\n",
    "        # evaluation on validation set\n",
    "        dev_acc = eval(val_dataloader, model, args)\n",
    "        if dev_acc > best_acc:\n",
    "            best_acc = dev_acc\n",
    "            last_step = steps\n",
    "#                     if args.save_best:\n",
    "#                         save(model, args.save_dir, 'best', steps)\n",
    "        else:\n",
    "            if steps - last_step >= args.early_stop:\n",
    "                print('early stop by {} steps.'.format(args.early_stop))\n",
    "#             elif steps % args.save_interval == 0:\n",
    "#                 save(model, args.save_dir, 'snapshot', steps)\n",
    "\n",
    "\n",
    "def predict(text, model, text_field, label_feild, cuda_flag):\n",
    "    assert isinstance(text, str)\n",
    "    model.eval()\n",
    "    # text = text_field.tokenize(text)\n",
    "    text = text_field.preprocess(text)\n",
    "    text = [[text_field.vocab.stoi[x] for x in text]]\n",
    "    x = torch.tensor(text)\n",
    "    x = autograd.Variable(x)\n",
    "    if cuda_flag:\n",
    "        x = x.cuda()\n",
    "    print(x)\n",
    "    output = model(x)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    #return label_feild.vocab.itos[predicted.data[0][0]+1]\n",
    "    return label_feild.vocab.itos[predicted.data[0]+1]\n",
    "\n",
    "\n",
    "# def save(model, save_dir, save_prefix, steps):\n",
    "#     if not os.path.isdir(save_dir):\n",
    "#         os.makedirs(save_dir)\n",
    "#     save_prefix = os.path.join(save_dir, save_prefix)\n",
    "#     save_path = '{}_steps_{}.pt'.format(save_prefix, steps)\n",
    "#     torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, parameters):\n",
    "        # model parameters\n",
    "        self.lr = parameters['lr'] if parameters.get('lr') != None else 0.001\n",
    "        self.epochs = parameters['epoch'] if parameters.get('epoch') != None else 10\n",
    "        self.batch_size = parameters['batch_size'] if parameters.get('batch_size') != None else 64\n",
    "        self.shuffle = parameters['shuffle'] if parameters.get('shuffle') != None else False                     # whether to shuffle data after every epoch\n",
    "        self.dropout = parameters['dropout'] if parameters.get('dropout') != None else 0.5\n",
    "        self.max_norm = parameters['l2-norm'] if parameters.get('l2-norm') != None else 3.0                      # l2 norm\n",
    "        self.embed_dim = parameters['embed_dim'] if parameters.get('embed_dim') != None else 300                 # embedding dimension\n",
    "        self.embed_num = parameters['embed_num'] if parameters.get('embed_num') != None else 5000               # number of filters for each conv\n",
    "        self.kernel_num = parameters['kernel_num'] if parameters.get('kernel_num') != None else 10               # number of filters for each conv\n",
    "        self.kernel_sizes = parameters['kernel_sizes'] if parameters.get('kernel_sizes') != None else [3,4,5]    # list of kernel sizes\n",
    "        self.class_num = parameters['class_num'] if parameters.get('class_num') != None else 2\n",
    "        \n",
    "        # training precocess parameters\n",
    "        self.log_interval = parameters['log_interval'] if  parameters.get('log_interval') != None else 1\n",
    "        self.test_interval = parameters['test_interval'] if  parameters.get('test_interval') != None else 10\n",
    "        self.save_interval = parameters['save_interval'] if  parameters.get('save_interval') != None else 100\n",
    "        self.save_dir = parameters['save_dir'] if  parameters.get('save_dir') != None else './'\n",
    "        self.early_stop = parameters['early_stop'] if  parameters.get('early_stop') != None else 1000\n",
    "        self.save_best = parameters['save_best'] if  parameters.get('save_best') != None else True\n",
    "        self.no_cuda = parameters['no_cuda'] if  parameters.get('no_cuda') != None else True            # wether to use gpu\n",
    "        self.device = parameters['device'] if  parameters.get('device') != None else -1                 # which device to use -1 means cpu\n",
    "        self.cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] - loss: 0.713753\n",
      "Evaluation - loss: 0.668496  acc: 57.0000%(82/143) \n",
      "\n",
      "Epoch[2] - loss: 0.486003\n",
      "Evaluation - loss: 0.634749  acc: 62.0000%(89/143) \n",
      "\n",
      "Epoch[3] - loss: 0.369082\n",
      "Evaluation - loss: 0.623375  acc: 65.0000%(93/143) \n",
      "\n",
      "Epoch[4] - loss: 0.289246\n",
      "Evaluation - loss: 0.624355  acc: 65.0000%(93/143) \n",
      "\n",
      "Epoch[5] - loss: 0.235246\n",
      "Evaluation - loss: 0.633821  acc: 64.0000%(92/143) \n",
      "\n",
      "Epoch[6] - loss: 0.197215\n",
      "Evaluation - loss: 0.647443  acc: 63.0000%(91/143) \n",
      "\n",
      "Epoch[7] - loss: 0.168674\n",
      "Evaluation - loss: 0.663970  acc: 62.0000%(90/143) \n",
      "\n",
      "Epoch[8] - loss: 0.147588\n",
      "Evaluation - loss: 0.682554  acc: 62.0000%(89/143) \n",
      "\n",
      "Epoch[9] - loss: 0.132097\n",
      "Evaluation - loss: 0.702651  acc: 60.0000%(87/143) \n",
      "\n",
      "Epoch[10] - loss: 0.120645\n",
      "Evaluation - loss: 0.724343  acc: 60.0000%(87/143) \n",
      "\n",
      "Epoch[11] - loss: 0.112046\n",
      "Evaluation - loss: 0.745797  acc: 60.0000%(87/143) \n",
      "\n",
      "Epoch[12] - loss: 0.105420\n",
      "Evaluation - loss: 0.766113  acc: 60.0000%(87/143) \n",
      "\n",
      "Epoch[13] - loss: 0.100205\n",
      "Evaluation - loss: 0.785496  acc: 61.0000%(88/143) \n",
      "\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "parameters = {'class_num':2, 'epoch':20, 'batch_size':64 }\n",
    "args = Args(parameters)\n",
    "\n",
    "# change dir\n",
    "args.save_dir = os.path.join(args.save_dir, datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "# check gpu avialiable\n",
    "args.cuda = (not args.no_cuda) and torch.cuda.is_available()\n",
    "\n",
    "# data loader\n",
    "train_data = CodeChangeDataset(X_train, y_train, transform = ToTensor())\n",
    "test_data = CodeChangeDataset(X_test, y_test, transform = ToTensor())\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=args.batch_size,shuffle=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_data, batch_size=args.batch_size, shuffle = False, num_workers=4)\n",
    "\n",
    "# model\n",
    "cnn = CNN_Text(args)\n",
    "# if args.snapshot is not None:\n",
    "#     print('\\nLoading model from {}...'.format(args.snapshot))\n",
    "#     cnn.load_state_dict(torch.load(args.snapshot))\n",
    "\n",
    "# if args.cuda:\n",
    "#     torch.cuda.set_device(args.device)\n",
    "#     cnn = cnn.cuda()\n",
    "\n",
    "try:\n",
    "    train(train_dataloader, test_dataloader, cnn, args)\n",
    "except KeyboardInterrupt:\n",
    "    print('\\n' + '-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

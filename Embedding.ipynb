{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "from simplejson import JSONDecodeError\n",
    "from collections import Counter \n",
    "import pprint \n",
    "import math\n",
    "import argparse \n",
    "import os\n",
    "import collections\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "import time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(filepath):\n",
    "    \"\"\"\n",
    "    function used to parse json of each commit json file\n",
    "\n",
    "    Args:\n",
    "        filepath_list - list of filepaths\n",
    "\n",
    "    Returns:\n",
    "        files_json - list object contains parsed information\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    files_json = []\n",
    "\n",
    "    # each commits\n",
    "    files = os.listdir(filepath)\n",
    "    for path in files:\n",
    "        if os.stat(filepath + path).st_size != 0 and path != 'desktop.ini':\n",
    "            with open(filepath + path, encoding=\"utf8\") as f:\n",
    "                data = json.load(f)\n",
    "                files_list = []\n",
    "                # each file in commits\n",
    "                for file in data['files']:\n",
    "                    # parse only cluster file\n",
    "                    for key in file.keys():\n",
    "                        if re.match('^.*_cluster$', key):\n",
    "                            actions_list = []\n",
    "                            actions = file[key]['actions']\n",
    "                            # each action in file\n",
    "                            for action in actions:\n",
    "                                actions_list.append(action['root'])\n",
    "                            files_list.append(actions_list)\n",
    "            if len(files_list) != 0:\n",
    "                files_json.append(files_list)\n",
    "    # return\n",
    "    return files_json\n",
    "\n",
    "\n",
    "folder_path = 'C:\\\\Users\\\\ichel\\\\Desktop\\\\shared_ReFiles\\\\AllFiles_Research\\\\'\n",
    "all_files = parse_json(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_roots(files_data):\n",
    "    counting = {}\n",
    "    for file_index, files in enumerate(files_data):\n",
    "        for root_index, roots in enumerate(files):\n",
    "            for action_index, actions in enumerate(roots):\n",
    "                temp = actions.split(' at ')[0]\n",
    "                tempq = []\n",
    "                if temp.startswith('INS'):\n",
    "                    tempq.append('INS')\n",
    "                    words = [temp.split('INS ')[1].split('to ')[0].strip()\n",
    "                             ] + [temp.split('INS ')[1].split('to ')[-1].strip()]\n",
    "                    for items in words:\n",
    "\n",
    "                        items = items.split(': ')[0]\n",
    "                        tempq.append(items)\n",
    "                    temp = '_'.join(tempq)\n",
    "\n",
    "                if temp.startswith('UPDATE'):\n",
    "                    temp = 'UPDATE'\n",
    "                if temp.startswith('MOVE'):\n",
    "                    temp2 = temp.split(' from ')[1]\n",
    "                    tempq.append('MOVE')\n",
    "                    tempq.append(temp2.split(': ')[0])\n",
    "                    temp = '_'.join(tempq)\n",
    "\n",
    "                if temp.startswith('DEL'):\n",
    "                    tempq.append('DEL')\n",
    "                    tempq.append(temp.split('DEL ')[1].split(': ')[0])\n",
    "                    temp = '_'.join(tempq)\n",
    "                counting[temp] = counting.get(temp, 0) + 1\n",
    "                files_data[file_index][root_index][action_index] = temp\n",
    "    dic = {}\n",
    "    i = 0\n",
    "    for k, v in counting.items():\n",
    "        dic[k] = i  \n",
    "        i += 1\n",
    "    return dic, files_data, counting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic, datas, freq_dict = preprocess_roots(all_files)\n",
    "rev_dic = dict(zip(dic.values(), dic.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sample training data>:  ['INS_ImportDeclaration_CompilationUnit INS_SingleVariableDeclaration_MethodDeclaration INS_SingleVariableDeclaration_MethodDeclaration INS_TagElement_Javadoc INS_TagElement_Javadoc', 'INS_SingleVariableDeclaration_MethodDeclaration INS_SingleVariableDeclaration_MethodDeclaration INS_VariableDeclarationStatement_Block INS_VariableDeclarationStatement_Block INS_ExpressionStatement_Block INS_ExpressionStatement_Block INS_SimpleName_ReturnStatement MOVE_VariableDeclarationFragment', 'INS_SimpleName_MethodInvocation INS_SimpleName_MethodInvocation INS_SimpleName_MethodInvocation']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def actions2sentence(datas):\n",
    "    data_total = []\n",
    "    for files in datas:\n",
    "        data4file = []\n",
    "        for roots in files:\n",
    "            sentence = ' '.join(roots)\n",
    "            data4file.append(sentence)\n",
    "        data_total.append(data4file)\n",
    "    return data_total\n",
    "\n",
    "\n",
    "training_data = actions2sentence(datas)\n",
    "\n",
    "def get_seqlength(training_data):\n",
    "    max_root_len = 0\n",
    "    seqlength_list = []\n",
    "    for items in training_data:\n",
    "        for item in items:\n",
    "            seqlength_list.append(len(item.split(\" \")))\n",
    "            if len(item.split(\" \")) >  max_root_len: \n",
    "                max_root_len = len(item.split(\" \"))\n",
    "    return max_root_len, seqlength_list\n",
    "\n",
    "def plot_hist(seqlength_list): \n",
    "    plt.figure(figsize=(20,10))\n",
    "    number_of_files = np.array(seqlength_list)\n",
    "    bincount = np.bincount(seqlength_list)\n",
    "    x = np.arange(1, len(bincount)+1)\n",
    "    n, bins, patches = plt.hist(seqlength_list,x)\n",
    "\n",
    "max_seqlength, sequence_list = get_seqlength(training_data)\n",
    "print(\"<sample training data>: \", training_data[0])\n",
    "plot_hist(sequence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "INS_TextElement_begin\n",
      "validation\n",
      "INS_TextElement:_do\n",
      "so\n",
      "INS_TextElement_begin\n",
      "validation\n",
      "INS_TextElement_use\n",
      "this\n",
      "implementation\n",
      "by\n",
      "simply\n",
      "defining\n",
      "this\n",
      "file\n",
      "INS_TextElement_be\n",
      "successful\n",
      "during\n",
      "the\n",
      "attempt,\n",
      "or\n",
      "perhaps\n",
      "only\n",
      "INS_TextElement_the\n",
      "subject\n",
      "INS_TextElement_be\n",
      "dynamically\n",
      "resolved\n",
      "or\n",
      "construtected\n",
      "INS_TextElement_retrieve\n",
      "a\n",
      "script\n",
      "INS_TextElement_run\n",
      "the\n",
      "compiler\n",
      "in-process\n",
      "in\n",
      "Java,\n",
      "you\n",
      "should\n",
      "look\n",
      "INS_TextElement:_make\n",
      "assertions\n",
      "about\n",
      "its\n",
      "value\n",
      "INS_TextElement_AST\n",
      "Nodes\n",
      "INS_TextElement_traverse\n",
      "nodes\n",
      "beginning\n",
      "INS_TextElement:_make\n",
      "assertions\n",
      "about\n",
      "its\n",
      "value\n",
      "INS_TextElement_avoid\n",
      "invoking\n",
      "the\n",
      "soy\n",
      "compiler\n",
      "INS_TextElement_be\n",
      "done\n",
      "INS_TextElement_implement\n",
      "both\n",
      "interfaces\n",
      "INS_TextElement_implement\n",
      "both\n",
      "interfaces\n",
      "INS_TextElement_compute\n",
      "this\n",
      "list\n",
      "INS_TextElement_have\n",
      "weights\n",
      "for\n",
      "the\n",
      "buckets\n",
      "in\n",
      "the\n",
      "future,\n",
      "take\n",
      "a\n",
      "look\n",
      "INS_TextElement_have\n",
      "weights\n",
      "for\n",
      "the\n",
      "buckets\n",
      "in\n",
      "the\n",
      "future,\n",
      "take\n",
      "a\n",
      "look\n",
      "INS_TextElement_garbage\n",
      "collect\n",
      "this\n",
      "class\n",
      "loader,\n",
      "but\n",
      "INS_TextElement_validate\n",
      "a\n",
      "binding\n",
      "INS_TextElement_do\n",
      "that,\n",
      "there\n",
      "can\n",
      "only\n",
      "be\n",
      "one\n",
      "such\n",
      "thread\n",
      "INS_TextElement_new\n",
      "one\n",
      "or\n",
      "no\n",
      "longer\n",
      "INS_TextElement_the\n",
      "time\n",
      "partition.\n",
      "Only\n",
      "one\n",
      "partition\n",
      "is\n",
      "active\n",
      "INS_TextElement_the\n",
      "time\n",
      "partition.\n",
      "Only\n",
      "one\n",
      "partition\n",
      "is\n",
      "active\n",
      "INS_TextElement:_invoke\n",
      "all\n",
      "the\n",
      "@PreDestroy\n",
      "methods\n"
     ]
    }
   ],
   "source": [
    "def concat_train_data(training_data): \n",
    "    concat_data = \"\"\n",
    "    tmp_list = []\n",
    "    for items in training_data: \n",
    "        tmp_list += items\n",
    "    concat_data = \" \".join(tmp_list)\n",
    "    return concat_data\n",
    "    \n",
    "def tokenize_train_data(concat_train_data): \n",
    "    data = []\n",
    "    for word in concat_train_data.split():\n",
    "        index = dic.get(word)\n",
    "        if index is None: # for debug \n",
    "            print(word) # for debug \n",
    "        data.append(index)\n",
    "    return data\n",
    "\n",
    "tokenized_data = concat_train_data(training_data)\n",
    "print(type(tokenized_data))\n",
    "tokenized_data = tokenize_train_data(tokenized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling \n",
    "Here we apply subsampling to get rid of roots such as \"UPDATE\", \"INS_Method\", which show up frequently in the dataset \n",
    "and could affect training speed and also the quality if the embeddings. The subsampling employed here is by Mikolov the creator of word2vec. For each root ri in the training set, we discard it with probability given by p(ri) = 1 - sqrt(1/(f(ri))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1296754\n",
      "54028\n",
      "138\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "def subsample(tokenized_data): \n",
    "    \"\"\"\n",
    "    Remove frequently occuring roots for faster training and better representation. \n",
    "    For the subsampling I am implementing Mikolov's negative.\n",
    "    For each root ri it will discard it with the given probability p(ri) = 1 - sqrt(1/(f(ri)).\n",
    "    :param tokenized_data: A list that contains the position in of each root in all commits into a single document \n",
    "    \"\"\"\n",
    "    threshold = 1e-5\n",
    "    roots_count = Counter(tokenized_data)\n",
    "    total_count = len(tokenized_data)\n",
    "    freqs = {root: count/total_count for root, count in roots_count.items()}\n",
    "    p_drop = {root: 1 - np.sqrt(threshold/freqs[root]) for root in roots_count}\n",
    "    sampled_roots = [root for root in tokenized_data if random.random() < (1 - p_drop[root])]\n",
    "    return sampled_roots \n",
    "\n",
    "sampled_roots = subsample(tokenized_data)\n",
    "print(len(tokenized_data))\n",
    "print(len(sampled_roots))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(roots, idx, window_size=8): \n",
    "    \"\"\"\n",
    "    Here we set window_size = 5 and select randomly a number N in the range (1:N)\n",
    "    and then use N roots to current root and currnent + N roots in the future... reference roots as \"words\"\n",
    "    :param roots: batch of sampled tokenized roots \n",
    "    :param idx: batch index \n",
    "    :param window_size: skip-gram window size -- HYPERPARAMETER \n",
    "    \n",
    "    \"\"\"\n",
    "    N = np.random.randint(1, window_size + 1)\n",
    "    start = idx - N if (idx - N) > 0 else 0 \n",
    "    stop = idx + N\n",
    "    targets = set(roots[start:idx] + roots[idx+1:stop+1])\n",
    "    return list(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batchs(roots, batch_size, window_size=8): \n",
    "    n_batches = len(roots)//batch_size \n",
    "    \n",
    "    # make sure to select full batches \n",
    "    roots = roots[:n_batches * batch_size]\n",
    "    \n",
    "    for idx in range(0, len(roots), batch_size): \n",
    "        i, j = [], []\n",
    "        batch = roots[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)): \n",
    "            batch_i = batch[ii]\n",
    "            batch_j = get_targets(batch, ii, window_size)\n",
    "            i.extend([batch_i] * len(batch_j))\n",
    "            j.extend(batch_j)\n",
    "        yield i, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "length of roots: 54\n",
      "roots: \n",
      "[467, 467, 467, 467, 467, 554, 554, 554, 554, 554, 554, 554, 327, 327, 327, 327, 327, 327, 327, 327, 327, 327, 450, 450, 450, 450, 450, 450, 450, 450, 753, 450, 450, 450, 450, 450, 450, 450, 450, 730, 730, 730, 730, 730, 730, 59, 59, 59, 59, 59, 59, 229, 229, 229]\n",
      "---------------\n",
      "labels: \n",
      "[[450]\n",
      " [554]\n",
      " [753]\n",
      " [730]\n",
      " [327]\n",
      " [450]\n",
      " [229]\n",
      " [327]\n",
      " [753]\n",
      " [467]\n",
      " [730]\n",
      " [ 59]\n",
      " [450]\n",
      " [229]\n",
      " [327]\n",
      " [554]\n",
      " [753]\n",
      " [467]\n",
      " [730]\n",
      " [ 59]\n",
      " [450]\n",
      " [327]\n",
      " [450]\n",
      " [229]\n",
      " [327]\n",
      " [554]\n",
      " [753]\n",
      " [467]\n",
      " [730]\n",
      " [ 59]\n",
      " [450]\n",
      " [450]\n",
      " [229]\n",
      " [327]\n",
      " [554]\n",
      " [753]\n",
      " [467]\n",
      " [730]\n",
      " [ 59]\n",
      " [450]\n",
      " [229]\n",
      " [327]\n",
      " [554]\n",
      " [753]\n",
      " [ 59]\n",
      " [450]\n",
      " [229]\n",
      " [327]\n",
      " [554]\n",
      " [753]\n",
      " [730]\n",
      " [730]\n",
      " [450]\n",
      " [ 59]]\n",
      " \n",
      "epoch 0\n",
      "length of roots: 36\n",
      "roots: \n",
      "[373, 373, 373, 373, 463, 463, 463, 574, 574, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 607]\n",
      "---------------\n",
      "labels: \n",
      "[[248]\n",
      " [161]\n",
      " [574]\n",
      " [463]\n",
      " [161]\n",
      " [373]\n",
      " [574]\n",
      " [161]\n",
      " [463]\n",
      " [248]\n",
      " [161]\n",
      " [373]\n",
      " [574]\n",
      " [463]\n",
      " [161]\n",
      " [463]\n",
      " [373]\n",
      " [248]\n",
      " [574]\n",
      " [607]\n",
      " [248]\n",
      " [161]\n",
      " [161]\n",
      " [463]\n",
      " [373]\n",
      " [248]\n",
      " [574]\n",
      " [607]\n",
      " [248]\n",
      " [161]\n",
      " [574]\n",
      " [607]\n",
      " [248]\n",
      " [161]\n",
      " [607]\n",
      " [248]]\n",
      " \n",
      "epoch 0\n",
      "length of roots: 56\n",
      "roots: \n",
      "[592, 592, 592, 592, 592, 592, 335, 335, 335, 335, 335, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 364, 364, 364, 364, 364, 364, 364, 565, 565, 565, 565, 565, 364, 364, 364, 364, 364, 254, 254, 254, 254, 254, 260, 260, 260, 260, 260, 260, 443, 443, 443, 443, 443]\n",
      "---------------\n",
      "labels: \n",
      "[[260]\n",
      " [364]\n",
      " [335]\n",
      " [565]\n",
      " [443]\n",
      " [254]\n",
      " [592]\n",
      " [260]\n",
      " [364]\n",
      " [565]\n",
      " [254]\n",
      " [592]\n",
      " [364]\n",
      " [565]\n",
      " [254]\n",
      " [335]\n",
      " [260]\n",
      " [364]\n",
      " [335]\n",
      " [592]\n",
      " [565]\n",
      " [443]\n",
      " [254]\n",
      " [260]\n",
      " [364]\n",
      " [335]\n",
      " [592]\n",
      " [565]\n",
      " [443]\n",
      " [254]\n",
      " [443]\n",
      " [364]\n",
      " [260]\n",
      " [254]\n",
      " [335]\n",
      " [260]\n",
      " [443]\n",
      " [364]\n",
      " [565]\n",
      " [254]\n",
      " [260]\n",
      " [443]\n",
      " [364]\n",
      " [565]\n",
      " [254]\n",
      " [364]\n",
      " [335]\n",
      " [592]\n",
      " [565]\n",
      " [443]\n",
      " [254]\n",
      " [260]\n",
      " [364]\n",
      " [565]\n",
      " [254]\n",
      " [335]]\n",
      " \n",
      "epoch 0\n",
      "length of roots: 50\n",
      "roots: \n",
      "[727, 450, 450, 450, 450, 450, 450, 320, 320, 506, 506, 506, 506, 506, 506, 506, 506, 229, 229, 229, 229, 229, 229, 578, 578, 578, 578, 578, 578, 390, 390, 390, 390, 629, 629, 629, 629, 629, 629, 629, 574, 574, 574, 574, 574, 574, 574, 450, 450, 450]\n",
      "---------------\n",
      "labels: \n",
      "[[450]\n",
      " [320]\n",
      " [578]\n",
      " [229]\n",
      " [390]\n",
      " [727]\n",
      " [506]\n",
      " [450]\n",
      " [506]\n",
      " [320]\n",
      " [450]\n",
      " [578]\n",
      " [229]\n",
      " [390]\n",
      " [629]\n",
      " [727]\n",
      " [574]\n",
      " [320]\n",
      " [450]\n",
      " [578]\n",
      " [390]\n",
      " [629]\n",
      " [506]\n",
      " [320]\n",
      " [229]\n",
      " [390]\n",
      " [629]\n",
      " [506]\n",
      " [574]\n",
      " [578]\n",
      " [629]\n",
      " [229]\n",
      " [574]\n",
      " [320]\n",
      " [578]\n",
      " [450]\n",
      " [229]\n",
      " [390]\n",
      " [506]\n",
      " [574]\n",
      " [320]\n",
      " [578]\n",
      " [450]\n",
      " [229]\n",
      " [390]\n",
      " [629]\n",
      " [506]\n",
      " [574]\n",
      " [629]\n",
      " [390]]\n",
      " \n",
      "epoch 0\n",
      "length of roots: 58\n",
      "roots: \n",
      "[217, 450, 450, 450, 450, 450, 450, 450, 390, 390, 500, 500, 500, 500, 500, 500, 139, 139, 139, 139, 139, 139, 139, 139, 139, 457, 457, 457, 457, 457, 457, 457, 457, 457, 19, 19, 19, 19, 19, 19, 19, 19, 19, 244, 244, 244, 244, 244, 244, 244, 244, 244, 261, 261, 261, 261, 82, 82]\n",
      "---------------\n",
      "labels: \n",
      "[[450]\n",
      " [390]\n",
      " [457]\n",
      " [139]\n",
      " [ 19]\n",
      " [500]\n",
      " [244]\n",
      " [217]\n",
      " [450]\n",
      " [500]\n",
      " [450]\n",
      " [390]\n",
      " [457]\n",
      " [139]\n",
      " [ 19]\n",
      " [217]\n",
      " [450]\n",
      " [261]\n",
      " [390]\n",
      " [457]\n",
      " [ 82]\n",
      " [ 19]\n",
      " [500]\n",
      " [244]\n",
      " [217]\n",
      " [450]\n",
      " [261]\n",
      " [390]\n",
      " [139]\n",
      " [ 82]\n",
      " [ 19]\n",
      " [500]\n",
      " [244]\n",
      " [217]\n",
      " [450]\n",
      " [261]\n",
      " [390]\n",
      " [457]\n",
      " [139]\n",
      " [ 82]\n",
      " [500]\n",
      " [244]\n",
      " [217]\n",
      " [450]\n",
      " [261]\n",
      " [390]\n",
      " [457]\n",
      " [139]\n",
      " [ 82]\n",
      " [ 19]\n",
      " [500]\n",
      " [217]\n",
      " [457]\n",
      " [ 82]\n",
      " [ 19]\n",
      " [244]\n",
      " [244]\n",
      " [261]]\n",
      " \n",
      "epoch 0\n",
      "length of roots: 48\n",
      "roots: \n",
      "[749, 168, 168, 168, 168, 168, 168, 168, 315, 315, 315, 315, 315, 315, 315, 348, 348, 348, 348, 348, 83, 83, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 592, 592, 592, 592, 592, 592, 611, 611, 611, 611, 611, 611, 611]\n",
      "---------------\n",
      "labels: \n",
      "[[168]\n",
      " [611]\n",
      " [749]\n",
      " [592]\n",
      " [ 83]\n",
      " [315]\n",
      " [348]\n",
      " [ 94]\n",
      " [611]\n",
      " [168]\n",
      " [749]\n",
      " [592]\n",
      " [ 83]\n",
      " [348]\n",
      " [ 94]\n",
      " [168]\n",
      " [ 83]\n",
      " [315]\n",
      " [749]\n",
      " [ 94]\n",
      " [348]\n",
      " [ 94]\n",
      " [611]\n",
      " [168]\n",
      " [749]\n",
      " [592]\n",
      " [ 83]\n",
      " [315]\n",
      " [348]\n",
      " [ 94]\n",
      " [592]\n",
      " [611]\n",
      " [ 83]\n",
      " [348]\n",
      " [ 94]\n",
      " [592]\n",
      " [611]\n",
      " [ 94]\n",
      " [592]\n",
      " [611]\n",
      " [ 94]\n",
      " [168]\n",
      " [749]\n",
      " [592]\n",
      " [ 83]\n",
      " [315]\n",
      " [348]\n",
      " [ 94]]\n",
      " \n",
      "epoch 0\n",
      "length of roots: 59\n",
      "roots: \n",
      "[244, 244, 244, 654, 654, 654, 654, 654, 390, 390, 348, 348, 348, 348, 348, 348, 348, 348, 419, 419, 419, 419, 419, 419, 419, 419, 364, 364, 364, 364, 364, 364, 364, 364, 145, 145, 145, 145, 145, 145, 145, 145, 145, 145, 145, 145, 145, 145, 145, 145, 145, 596, 596, 596, 202, 202, 202, 202, 202]\n",
      "---------------\n",
      "labels: \n",
      "[[390]\n",
      " [348]\n",
      " [654]\n",
      " [364]\n",
      " [419]\n",
      " [244]\n",
      " [390]\n",
      " [348]\n",
      " [348]\n",
      " [654]\n",
      " [419]\n",
      " [390]\n",
      " [202]\n",
      " [364]\n",
      " [654]\n",
      " [145]\n",
      " [244]\n",
      " [596]\n",
      " [390]\n",
      " [202]\n",
      " [364]\n",
      " [654]\n",
      " [145]\n",
      " [244]\n",
      " [596]\n",
      " [348]\n",
      " [419]\n",
      " [390]\n",
      " [202]\n",
      " [654]\n",
      " [145]\n",
      " [244]\n",
      " [596]\n",
      " [348]\n",
      " [419]\n",
      " [390]\n",
      " [202]\n",
      " [364]\n",
      " [654]\n",
      " [145]\n",
      " [244]\n",
      " [596]\n",
      " [348]\n",
      " [419]\n",
      " [390]\n",
      " [202]\n",
      " [364]\n",
      " [654]\n",
      " [145]\n",
      " [596]\n",
      " [348]\n",
      " [145]\n",
      " [202]\n",
      " [364]\n",
      " [364]\n",
      " [145]\n",
      " [419]\n",
      " [348]\n",
      " [596]]\n",
      " \n",
      "epoch 0\n",
      "length of roots: 53\n",
      "roots: \n",
      "[1, 1, 1, 1, 1, 1, 703, 703, 450, 450, 450, 450, 450, 450, 450, 450, 450, 621, 621, 621, 621, 248, 248, 248, 248, 248, 248, 248, 450, 450, 450, 450, 450, 450, 109, 109, 109, 109, 109, 109, 179, 179, 179, 179, 179, 179, 248, 248, 248, 248, 248, 248, 248]\n",
      "---------------\n",
      "labels: \n",
      "[[450]\n",
      " [109]\n",
      " [621]\n",
      " [179]\n",
      " [248]\n",
      " [703]\n",
      " [  1]\n",
      " [450]\n",
      " [  1]\n",
      " [450]\n",
      " [621]\n",
      " [703]\n",
      " [248]\n",
      " [  1]\n",
      " [450]\n",
      " [621]\n",
      " [703]\n",
      " [248]\n",
      " [450]\n",
      " [109]\n",
      " [703]\n",
      " [  1]\n",
      " [450]\n",
      " [109]\n",
      " [621]\n",
      " [179]\n",
      " [248]\n",
      " [703]\n",
      " [450]\n",
      " [109]\n",
      " [621]\n",
      " [179]\n",
      " [248]\n",
      " [703]\n",
      " [  1]\n",
      " [450]\n",
      " [621]\n",
      " [179]\n",
      " [248]\n",
      " [703]\n",
      " [  1]\n",
      " [450]\n",
      " [109]\n",
      " [621]\n",
      " [248]\n",
      " [703]\n",
      " [  1]\n",
      " [450]\n",
      " [109]\n",
      " [621]\n",
      " [179]\n",
      " [248]\n",
      " [703]]\n",
      " \n",
      "epoch 0\n",
      "length of roots: 70\n",
      "roots: \n",
      "[767, 767, 767, 767, 767, 767, 767, 248, 248, 248, 248, 248, 248, 248, 248, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 54, 54, 54, 54, 54, 54, 54, 54, 574, 574, 574, 574, 574, 574, 574, 574, 435, 435, 435, 435, 435, 435, 147, 147, 147, 147, 147, 147, 147, 147, 82, 82, 82, 82, 82, 82, 620, 620, 620, 620]\n",
      "---------------\n",
      "labels: \n",
      "[[450]\n",
      " [ 82]\n",
      " [435]\n",
      " [147]\n",
      " [ 54]\n",
      " [248]\n",
      " [574]\n",
      " [450]\n",
      " [620]\n",
      " [ 82]\n",
      " [435]\n",
      " [147]\n",
      " [ 54]\n",
      " [574]\n",
      " [767]\n",
      " [450]\n",
      " [620]\n",
      " [ 82]\n",
      " [435]\n",
      " [147]\n",
      " [ 54]\n",
      " [248]\n",
      " [574]\n",
      " [767]\n",
      " [450]\n",
      " [435]\n",
      " [ 54]\n",
      " [248]\n",
      " [574]\n",
      " [767]\n",
      " [450]\n",
      " [620]\n",
      " [ 82]\n",
      " [435]\n",
      " [147]\n",
      " [248]\n",
      " [574]\n",
      " [767]\n",
      " [450]\n",
      " [620]\n",
      " [ 82]\n",
      " [435]\n",
      " [147]\n",
      " [ 54]\n",
      " [248]\n",
      " [767]\n",
      " [450]\n",
      " [620]\n",
      " [ 82]\n",
      " [147]\n",
      " [ 54]\n",
      " [574]\n",
      " [450]\n",
      " [620]\n",
      " [ 82]\n",
      " [435]\n",
      " [ 54]\n",
      " [248]\n",
      " [574]\n",
      " [767]\n",
      " [450]\n",
      " [620]\n",
      " [435]\n",
      " [147]\n",
      " [ 54]\n",
      " [574]\n",
      " [ 82]\n",
      " [435]\n",
      " [147]\n",
      " [574]]\n",
      " \n",
      "epoch 0\n",
      "length of roots: 65\n",
      "roots: \n",
      "[396, 396, 396, 396, 396, 396, 396, 396, 396, 254, 254, 254, 443, 443, 443, 443, 443, 443, 443, 443, 443, 315, 315, 315, 315, 315, 315, 315, 770, 770, 770, 770, 770, 770, 770, 770, 770, 91, 91, 324, 324, 324, 324, 324, 324, 324, 324, 324, 320, 320, 320, 320, 320, 320, 320, 538, 538, 538, 538, 538, 538, 538, 538, 538, 6]\n",
      "---------------\n",
      "labels: \n",
      "[[320]\n",
      " [770]\n",
      " [324]\n",
      " [  6]\n",
      " [315]\n",
      " [443]\n",
      " [538]\n",
      " [ 91]\n",
      " [254]\n",
      " [315]\n",
      " [443]\n",
      " [396]\n",
      " [320]\n",
      " [770]\n",
      " [324]\n",
      " [  6]\n",
      " [396]\n",
      " [315]\n",
      " [538]\n",
      " [ 91]\n",
      " [254]\n",
      " [320]\n",
      " [770]\n",
      " [324]\n",
      " [396]\n",
      " [443]\n",
      " [ 91]\n",
      " [254]\n",
      " [320]\n",
      " [324]\n",
      " [  6]\n",
      " [ 91]\n",
      " [396]\n",
      " [443]\n",
      " [538]\n",
      " [315]\n",
      " [254]\n",
      " [770]\n",
      " [324]\n",
      " [320]\n",
      " [770]\n",
      " [  6]\n",
      " [396]\n",
      " [315]\n",
      " [443]\n",
      " [538]\n",
      " [ 91]\n",
      " [254]\n",
      " [770]\n",
      " [324]\n",
      " [  6]\n",
      " [315]\n",
      " [443]\n",
      " [538]\n",
      " [ 91]\n",
      " [320]\n",
      " [770]\n",
      " [324]\n",
      " [  6]\n",
      " [396]\n",
      " [315]\n",
      " [443]\n",
      " [ 91]\n",
      " [254]\n",
      " [538]]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def print_sample_batch(tokenized_data=sampled_roots[:100]): \n",
    "    for ii in range(1): \n",
    "        batches = generate_batchs(tokenized_data, 10, 10)\n",
    "        for i, j in batches: \n",
    "            print(\"epoch {}\".format(ii))\n",
    "            print(\"length of roots: {}\".format(len(i)))\n",
    "            print(\"roots: \")\n",
    "            print(i)\n",
    "            print(\"---------------\")\n",
    "            print(\"labels: \")\n",
    "            print(np.array(j)[:, None])\n",
    "            print(\" \")\n",
    "print_sample_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "vocab_size = len(dic)\n",
    "rate = 0.4 # dropout_rate = 1 - rate \n",
    "embedding_size = 128  # number of embedding features \n",
    "num_sampled = 64 # number of negative examples to sample\n",
    "epochs = 100000\n",
    "batch_size = 128\n",
    "window_size = 10\n",
    "\n",
    "#Select random sample of most frequent words for model validation \n",
    "valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Network \n",
    "Input words are passed in as one-hot encoded vectors, which feed into a single hidden linear layer. \n",
    "The output of the embedding matrix of this network will then be fed into a CNN for prediction later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Graph\n",
    "We have varying input sizes so we set the dimenision to **None** and the second dimension of the \n",
    "labels to **None** as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default(): \n",
    "    inputs = tf.placeholder(tf.int32, [None], name=\"inputs\")\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name=\"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding \n",
    "Here the embedding matrix has the dimensions --> (tokenized_data X hidden_layers == embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with train_graph.as_default(): \n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embedding_size), -1, 1))\n",
    "    embedding = tf.nn.dropout(embedding, keep_prob=1 - rate, noise_shape=[vocab_size, 1])\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with train_graph.as_default(): \n",
    "    nce_weights = tf.Variable(tf.truncated_normal([vocab_size, embedding_size], stddev=1.0/math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocab_size]))\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(\n",
    "            weights=nce_weights, \n",
    "            biases=nce_biases, \n",
    "            labels=labels, \n",
    "            inputs=embed, \n",
    "            num_sampled=num_sampled, \n",
    "            num_classes=vocab_size\n",
    "        )\n",
    "    )\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "pick most common and least common roots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with train_graph.as_default(): \n",
    "    ## From Thusan Ganegedara's implementation \n",
    "    \n",
    "    # pick top 8 samples from (0, 100) and (600, 700) each ranges. lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples, random.sample(range(100, 600+valid_window), valid_size//2))\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # use cosine distance \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keepdims=True))\n",
    "    normalized_embedding = embedding/norm \n",
    "    valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file checkpoints already exists.\n"
     ]
    }
   ],
   "source": [
    "# if the checkpoints directory does not exist then make it \n",
    "!mkdir checkpoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-4f51a89d16c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mfeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ichel\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ichel\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32mc:\\users\\ichel\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "with train_graph.as_default(): \n",
    "    saver = tf.train.Saver()\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    step = 1\n",
    "    avg_loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for ii in range(1, epochs+1): \n",
    "        batches = generate_batchs(sampled_roots, batch_size, window_size)\n",
    "        start = time.time()\n",
    "        for i, j in batches: \n",
    "            feed = {inputs: i, labels: np.array(j)[:, None]}\n",
    "            train_loss, _ = sess.run([loss, optimizer], feed_dict=feed)\n",
    "            avg_loss += train_loss \n",
    "            if step % 100 == 0: \n",
    "                # Mat Leonard implementation \n",
    "                end = time.time()\n",
    "                print(\"Epoch {}/{}\".format(ii, epochs), \n",
    "                     \"Iteration: {}\".format(step), \n",
    "                     \"Avg. Training loss: {:.4f}\".format(avg_loss/100), \n",
    "                     \"{:.4f} sec/batch\".format((end - start)/100))\n",
    "                avg_loss = 0\n",
    "                start = time.time()\n",
    "            if step % 10000 == 0: \n",
    "                sim = similarity.eval()\n",
    "                for idx in range(valid_size): \n",
    "                    valid_word = rev_dic[valid_examples[idx]]\n",
    "                    top_k = 8\n",
    "                    nearest = (-sim[idx, :]).argsort()[1:top_k+1]\n",
    "                    log = \"Nearest to %s:\" % valid_word \n",
    "                    for k in range(top_k):\n",
    "                        close_word = rev_dic[nearest[k]]\n",
    "                        log = \"%s %s\" % (log, close_word)\n",
    "                    print(log)\n",
    "            step += 1\n",
    "    save_path = saver.save(sess, \"checkpoints/word_embeddings.ckpt\")\n",
    "    embed_mat = sess.run(normalized_embedding)\n",
    "                    \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

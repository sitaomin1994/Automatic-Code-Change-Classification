{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:43.628313Z",
     "start_time": "2019-07-21T03:27:34.819524Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "import sys\n",
    "from collections import Counter \n",
    "import pprint \n",
    "import math\n",
    "import argparse \n",
    "import os\n",
    "import collections\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "import time \n",
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:46.300541Z",
     "start_time": "2019-07-21T03:27:43.631478Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_json(filepath, files):\n",
    "    \"\"\"\n",
    "    function used to parse json of each commit json file\n",
    "\n",
    "    Args:\n",
    "        filepath_list - list of filepaths\n",
    "\n",
    "    Returns:\n",
    "        files_json - list object contains parsed information\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    files_json = []\n",
    "    commit_ids = []\n",
    "    # each commits\n",
    "    #files = os.listdir(filepath)\n",
    "    for path in files:\n",
    "        commit_id = path.split(\"_\")[1].split(\".\")[0]\n",
    "        try:\n",
    "            if os.stat(filepath + path).st_size != 0 and path != 'desktop.ini':\n",
    "                with open(filepath + path, encoding=\"utf8\") as f:\n",
    "                    data = json.load(f)\n",
    "                    files_list = []\n",
    "                    # each file in commits\n",
    "                    for file in data['files']:\n",
    "                        # parse only cluster file\n",
    "                        for key in file.keys():\n",
    "                            if re.match('^.*_cluster$', key):\n",
    "                                actions_list = []\n",
    "                                actions = file[key]['actions']\n",
    "                                # each action in file\n",
    "                                for action in actions:\n",
    "                                    actions_list.append(action['root'])\n",
    "                                files_list.append(actions_list)\n",
    "                if len(files_list) != 0:\n",
    "                    files_json.append(files_list)\n",
    "                    commit_ids.append(commit_id)\n",
    "        except FileNotFoundError as e: \n",
    "            continue \n",
    "    assert(len(commit_ids) == len(files_json))      \n",
    "    # return\n",
    "    return files_json, commit_ids\n",
    "\n",
    "\n",
    "folder_path = 'C:\\\\Users\\\\ichel\\\\Desktop\\\\tmp_JSON_labeled_commits\\\\'\n",
    "all_files, csha = parse_json(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:46.324762Z",
     "start_time": "2019-07-21T03:27:46.305599Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_roots(files_data):\n",
    "    counting = {}\n",
    "    for file_index, files in enumerate(files_data):\n",
    "        for root_index, roots in enumerate(files):\n",
    "            for action_index, actions in enumerate(roots):\n",
    "                temp = actions.split(' at ')[0].strip()\n",
    "                tempq = []\n",
    "                if temp.startswith('INS'):\n",
    "                    tempq.append('INS')\n",
    "                    words = [temp.split('INS ')[1].split('to ')[0].strip()] + [\n",
    "                        temp.split('INS ')[1].rsplit('to ')[-1].strip()\n",
    "                    ]\n",
    "                    for items in words:\n",
    "                        items = items.split(':')[0].strip()\n",
    "                        tempq.append(items)\n",
    "                    if tempq[1] == 'TextElement' and tempq[-1] not in ['TagElement', 'TextElement']:\n",
    "                        tempq[-1] = ''\n",
    "                    temp = '_'.join(tempq)\n",
    "\n",
    "                if temp.startswith('UPDATE'):\n",
    "                    temp = 'UPDATE'\n",
    "                if temp.startswith('MOVE'):\n",
    "                    temp2 = temp.split('from ')[1].strip()\n",
    "                    tempq.append('MOVE')\n",
    "                    tempq.append(temp2.split(':')[0].strip())\n",
    "                    temp = '_'.join(tempq)\n",
    "\n",
    "                if temp.startswith('DEL'):\n",
    "                    tempq.append('DEL')\n",
    "                    tempq.append(temp.split('DEL ')[1].split(':')[0].strip())\n",
    "                    temp = '_'.join(tempq)\n",
    "                temp = temp.replace(' ', '_')\n",
    "                counting[temp] = counting.get(temp, 0) + 1\n",
    "                files_data[file_index][root_index][action_index] = temp\n",
    "    dic = {}\n",
    "    i = 0\n",
    "    for k, v in counting.items():\n",
    "        dic[k] = i\n",
    "        i += 1\n",
    "    return dic, files_data, counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:46.475539Z",
     "start_time": "2019-07-21T03:27:46.328083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n"
     ]
    }
   ],
   "source": [
    "dic, datas, freq_dict = preprocess_roots(all_files)\n",
    "rev_dic = dict(zip(dic.values(), dic.keys()))\n",
    "print(len(dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:46.488683Z",
     "start_time": "2019-07-21T03:27:46.478368Z"
    }
   },
   "outputs": [],
   "source": [
    "def actions2sentence(datas):\n",
    "    data_total = []\n",
    "    for files in datas:\n",
    "        data4file = []\n",
    "        for roots in files:\n",
    "            sentence = ' '.join(roots)\n",
    "            data4file.append(sentence)\n",
    "        data_total.append(data4file)\n",
    "    return data_total\n",
    "\n",
    "\n",
    "training_data = actions2sentence(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_index_strange_words(training_data):\n",
    "    for i, item in enumerate(training_data):\n",
    "        for j, file in enumerate(item):\n",
    "            for k, action in enumerate(file.split()):\n",
    "                index = dic.get(action)\n",
    "                if index is None: # for debug \n",
    "                    print(\"==============================\")\n",
    "                    print(action) # for debug \n",
    "                    print('commits index: %d'%i)\n",
    "                    print('file: %d'%j)\n",
    "                    print('action: %d'%k)\n",
    "check_index_strange_words(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permute Order of File \n",
    "**Purpose**: Here we permutate the order of files within a given commit while still maintaining the labels for that commit. \n",
    "This helps our CNN by: \n",
    "- 1) Increasing the training samples \n",
    "- 2) Make the CNN invariant to file location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop labels\n",
    "def drop_labels(df, labels):\n",
    "    \"\"\"\n",
    "    Drop some of labels\n",
    "\n",
    "    Args:\n",
    "    df - Dataframs\n",
    "    labels - List of labels name to drop\n",
    "\n",
    "    Returns:\n",
    "    new_df -  new dataframe\n",
    "    \"\"\"\n",
    "    # remove labels in categories list\n",
    "    new_df = df.copy()\n",
    "    new_df['categories'] = new_df['categories'].apply(lambda row: [item for item in row if item not in labels])\n",
    "\n",
    "    # remove columns\n",
    "    new_df = new_df.drop(labels, axis=1)\n",
    "\n",
    "    # remove columns which have no labels after removing labels\n",
    "    new_df['number_of_labels'] = new_df['categories'].apply(lambda row: len(row))\n",
    "    new_df = new_df[new_df['number_of_labels'] != 0].reset_index(drop=True)\n",
    "    new_df = new_df.drop(['number_of_labels'], axis=1)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group some of labels\n",
    "def group_labels(df, labels_to_group, new_label):\n",
    "    '''\n",
    "    Group some of labels\n",
    "\n",
    "    Args:\n",
    "        df - dataframe\n",
    "        labels_to_group -  List of labels you want to group\n",
    "        new_label -  string - new label name of grouped labels\n",
    "\n",
    "    Returns:\n",
    "        new_df - dataframe after grouped\n",
    "    '''\n",
    "    new_df = df.copy()\n",
    "\n",
    "    # generate new labels by group labels\n",
    "    def create_new_label(row, labels):\n",
    "        new_label = 0  # initialize new label\n",
    "        for label in labels:\n",
    "            if row[label] == 1:\n",
    "                new_label = 1  # if one of labels in grouped labels is 1 the new label is 1\n",
    "        return new_label\n",
    "\n",
    "    new_df[new_label] = df.apply(lambda row: create_new_label(row, labels_to_group), axis=1)\n",
    "\n",
    "    # drop old labels\n",
    "    new_df = new_df.drop(labels_to_group, axis=1)\n",
    "    \n",
    "    # generate list of new_categories\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation: \n",
    "Prepare data for embedding and training ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutate_files(csha, training_data): \n",
    "    commits_dic = dict()\n",
    "    for sha, training_file in zip(csha, training_data): \n",
    "        commits_dic[sha] = []\n",
    "        if len(training_file) <= 6: \n",
    "            tmp_permutate = list(itertools.permutations(training_file))\n",
    "            for permutated_file in tmp_permutate: \n",
    "                commits_dic[sha].append(list(permutated_file))\n",
    "        else: \n",
    "            commits_dic[sha].append(training_file)\n",
    "    return commits_dic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_list(commits_labels_df):\n",
    "    s= commits_labels_df.apply(lambda x: pd.Series(x['Files']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "    s.name = \"Files\"\n",
    "    commits_labels_df = commits_labels_df.drop(\"Files\", axis=1) \n",
    "    commits_labels_df = commits_labels_df.join(s)\n",
    "    return commits_labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1922, 28)\n",
      "<class 'list'>\n",
      "['Testing', 'Bug fix']\n",
      "exp_train_df shape: (18381, 13)\n",
      "train_df shape: (624, 13)\n",
      "test_df shape: (153, 13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Commit ID', 'Files', 'project name', 'commit_message', 'Maintenance',\n",
       "       'Feature Add', 'Bug fix', 'Clean up', 'Refactoring', 'Token Replace',\n",
       "       'categories', 'Cross_', 'Documentation_'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commits_df = pd.DataFrame(data = [csha, training_data]).T\n",
    "commits_df.columns = [\"Commit ID\", \"Files\"]\n",
    "df_new = pd.read_csv('C:\\\\Users\\\\ichel\\\\Desktop\\\\commit_data_new.csv')\n",
    "print(df_new.shape)\n",
    "\n",
    "# convert string to list\n",
    "from ast import literal_eval\n",
    "\n",
    "df_new['categories'] = df_new['categories'].apply(lambda x: literal_eval(x))\n",
    "print(type(df_new['categories'].values[0]))\n",
    "print(df_new['categories'].values[0])\n",
    "df_new = df_new.drop(['Unnamed: 0'], axis = 1)\n",
    "commits_labels_df = pd.merge(commits_df, df_new, on='Commit ID')\n",
    "commits_labels_df.head(1)\n",
    "commits_labels_df = drop_labels(commits_labels_df, [\"Testing\", \"Build\", \"Versioning\", \"Indentation\", \"Internationalization\", \"Merge\", \\\n",
    "                                                   \"Module Move\", \"Module Remove\", \"Source Control\", \"Rename\", \"Initialization\", \\\n",
    "                                                   \"Module Add\", \"Data\"])\n",
    "commits_labels_df = group_labels(commits_labels_df, [\"Cross\", \"Debug\"], \"Cross_\")\n",
    "commits_labels_df = group_labels(commits_labels_df, [\"Legal\", \"Documentation\"], \"Documentation_\")\n",
    "commits_labels_df.shape\n",
    "\n",
    "msk = np.random.rand(len(commits_labels_df)) < 0.8\n",
    "train_df = commits_labels_df[msk]\n",
    "test_df = commits_labels_df[~msk]\n",
    "\n",
    "permutate_train_dic = permutate_files(train_df['Commit ID'],train_df['Files'])\n",
    "permutate_train_df = pd.DataFrame(data = permutate_train_dic.items(), columns=['Commit ID','Files'])\n",
    "train_df = train_df.drop([\"Files\"], axis=1)\n",
    "train_df['Files'] = permutate_train_df['Files'].values\n",
    "expanded_train_df = expand_list(train_df)\n",
    "\n",
    "print('exp_train_df shape:',expanded_train_df.shape)\n",
    "print('train_df shape:',train_df.shape)\n",
    "print('test_df shape:',test_df.shape)\n",
    "commit_files = commits_labels_df[\"Files\"].values\n",
    "commits_labels_df.columns\n",
    "\n",
    "# commits_test_dic = dict()\n",
    "# for sha, training_file in zip(csha, training_data): \n",
    "#     commits_dic[sha] = []\n",
    "#     commits_dic[sha].append(training_file)\n",
    "    \n",
    "# commits_df = pd.DataFrame(commits_dic.items())\n",
    "# commits_df.columns = [\"Commit ID\", \"Files\"]\n",
    "\n",
    "# df_new = pd.read_csv('C:\\\\Users\\\\ichel\\\\Desktop\\\\commit_data_new.csv')\n",
    "# print(df_new.shape)\n",
    "\n",
    "# # convert string to list\n",
    "# from ast import literal_eval\n",
    "\n",
    "# df_new['categories'] = df_new['categories'].apply(lambda x: literal_eval(x))\n",
    "# print(type(df_new['categories'].values[0]))\n",
    "# print(df_new['categories'].values[0])\n",
    "# df_new = df_new.drop(['Unnamed: 0'], axis = 1)\n",
    "# commits_labels_df = pd.merge(commits_df, df_new, on='Commit ID')\n",
    "# commits_labels_df.head(1)\n",
    "# s= commits_labels_df.apply(lambda x: pd.Series(x['Files']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "# s.name = \"Files\"\n",
    "# commits_labels_df = commits_labels_df.drop(\"Files\", axis=1) \n",
    "# commits_labels_df = commits_labels_df.join(s)\n",
    "# commits_labels_df = drop_labels(commits_labels_df, [\"Testing\", \"Build\", \"Versioning\", \"Indentation\", \"Internationalization\", \"Merge\", \\\n",
    "#                                                    \"Module Move\", \"Module Remove\", \"Source Control\", \"Rename\", \"Initialization\", \\\n",
    "#                                                    \"Module Add\", \"Data\"])\n",
    "# commits_labels_df = group_labels(commits_labels_df, [\"Cross\", \"Debug\"], \"Cross_\")\n",
    "# commits_labels_df = group_labels(commits_labels_df, [\"Legal\", \"Documentation\"], \"Documentation_\")\n",
    "# commits_labels_df.shape\n",
    "# commit_files = commits_labels_df[\"Files\"].values\n",
    "# commits_labels_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:46.502749Z",
     "start_time": "2019-07-21T03:27:46.491433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INS_NullLiteral_VariableDeclarationFragment INS_IfStatement_Block INS_Block_IfStatement MOVE_Block MOVE_IfStatement INS_VariableDeclarationStatement_Block INS_VariableDeclarationStatement_Block INS_IfStatement_Block MOVE_VariableDeclarationFragment MOVE_VariableDeclarationFragment MOVE_VariableDeclarationFragment INS_SimpleName_MethodInvocation INS_SimpleName_MethodInvocation INS_SimpleName_MethodInvocation MOVE_ClassInstanceCreation DEL_ClassInstanceCreation\n",
      "  \n",
      "UPDATE UPDATE\n"
     ]
    }
   ],
   "source": [
    "def concat_files_to_sentence(expanded_train_list): \n",
    "    concat_data = \"\"\n",
    "    tmp_list = []\n",
    "    for items in expanded_train_list:\n",
    "        concat_data = \" \".join(items)\n",
    "        tmp_list.append(concat_data)\n",
    "    return tmp_list\n",
    "concat_train_data = concat_files_to_sentence(expanded_train_df[\"Files\"])\n",
    "concat_test_data = concat_files_to_sentence(test_df[\"Files\"])\n",
    "print(concat_train_data[0])\n",
    "print(\"  \")\n",
    "print(concat_test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine File Threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.112668Z",
     "start_time": "2019-07-21T03:27:46.510367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sample training data>:  ['UPDATE UPDATE']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_seqlength(training_data):\n",
    "    max_root_len = 0\n",
    "    seqlength_list = []\n",
    "    for item in training_data:\n",
    "        seqlength_list.append(len(item.split()))\n",
    "        if len(item.split()) >  max_root_len: \n",
    "            max_root_len = len(item.split())\n",
    "    return max_root_len, seqlength_list\n",
    "\n",
    "def plot_hist(seqlength_list): \n",
    "    plt.figure(figsize=(20,10))\n",
    "    number_of_files = np.array(seqlength_list)\n",
    "    bincount = np.bincount(seqlength_list)\n",
    "    x = np.arange(1, len(bincount)+1)\n",
    "    n, bins, patches = plt.hist(seqlength_list,x)\n",
    "    plt.xlim((0, 500))\n",
    "    plt.ylim((0, 2000))\n",
    "\n",
    "max_seqlength, sequence_list = get_seqlength(concat_train_data)\n",
    "print(\"<sample training data>: \", training_data[0])\n",
    "plot_hist(sequence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.136546Z",
     "start_time": "2019-07-21T03:27:53.117407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n"
     ]
    }
   ],
   "source": [
    "# getting file threshold\n",
    "threshold = 0.95\n",
    "number_of_actions = [len(item.split()) for item in concat_train_data]\n",
    "\n",
    "def get_file_threshold(number_of_files, threshold = 0.95):\n",
    "    '''\n",
    "    get padding threshold for files dimension\n",
    "    \n",
    "    Args:\n",
    "        number_of_files - array of the number of files in each commits\n",
    "        threshold - drop all commits with its the number of files beyond this threshold\n",
    "    Returns:\n",
    "        padding threshold - number\n",
    "    '''\n",
    "    \n",
    "    total_files = len(number_of_files)\n",
    "    number_of_files = np.array(number_of_files)\n",
    "    bincount = np.bincount(number_of_files)\n",
    "\n",
    "    sum_file = 0\n",
    "    for index, item in enumerate(bincount):\n",
    "        sum_file += item\n",
    "        #print(index,item)\n",
    "        #print(sum_file)\n",
    "        if sum_file > threshold*total_files:\n",
    "            padding_files_threshold = index\n",
    "            break\n",
    "            \n",
    "    return padding_files_threshold\n",
    "\n",
    "length_threshold = get_file_threshold(number_of_actions, threshold)\n",
    "print(length_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_train_df = expanded_train_df.drop([\"Files\"], axis=1)\n",
    "test_df = test_df.drop([\"Files\"], axis=1)\n",
    "expanded_train_df[\"Files\"] = concat_train_data \n",
    "test_df[\"Files\"] = concat_test_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.530703Z",
     "start_time": "2019-07-21T03:27:53.490766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Commit ID', 'project name', 'commit_message', 'Maintenance',\n",
      "       'Feature Add', 'Bug fix', 'Clean up', 'Refactoring', 'Token Replace',\n",
      "       'categories', 'Cross_', 'Documentation_', 'Files', 'len_seq'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "expanded_train_df['len_seq'] = expanded_train_df.apply(lambda row: len(row['Files'].split()), axis = 1)\n",
    "test_df['len_seq'] = test_df.apply(lambda row: len(row['Files'].split()), axis = 1)\n",
    "expanded_train_df = expanded_train_df[expanded_train_df['len_seq'] <= length_threshold].reset_index(drop = True)\n",
    "test_df = test_df[test_df['len_seq'] <= length_threshold].reset_index(drop = True)\n",
    "print(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.960319Z",
     "start_time": "2019-07-21T03:27:47.213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traing labels shape:  (17869,)\n",
      "test labels shape:  (132,)\n"
     ]
    }
   ],
   "source": [
    "target_col = [\"Maintenance\", \"Feature Add\", \"Bug fix\", \"Clean up\", \"Refactoring\", \"Token Replace\", \"Cross_\", \"Documentation_\"]\n",
    "y_train = expanded_train_df[\"Maintenance\"].values\n",
    "y_test = test_df[\"Maintenance\"].values\n",
    "print(\"traing labels shape: \", y_train.shape) \n",
    "print(\"test labels shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Pad data \n",
    "We tokenize the data and pad with the token <PAD/>.<br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.966249Z",
     "start_time": "2019-07-21T03:27:47.781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[191, 28, 61, 12, 24, 13, 13, 28, 35, 35, 35, 8, 8, 8, 22, 49]\n",
      "[1, 1]\n",
      "(17869, 144)\n",
      "(132, 144)\n"
     ]
    }
   ],
   "source": [
    "#Training \n",
    "train_docs = expanded_train_df['Files'].values\n",
    "t_train = Tokenizer(filters = '', lower=False)\n",
    "t_train.fit_on_texts(train_docs)\n",
    "\n",
    "#Testing \n",
    "test_docs = test_df['Files'].values \n",
    "t_test = Tokenizer(filters = '', lower=False)\n",
    "t_test.fit_on_texts(test_docs)\n",
    "\n",
    "sequences_train = t_train.texts_to_sequences(train_docs)\n",
    "sequences_test = t_test.texts_to_sequences(test_docs)\n",
    "print(sequences_train[0])\n",
    "print(sequences_test[0])\n",
    "\n",
    "#Pad training data \n",
    "padded_seq_train = pad_sequences(sequences_train, maxlen=length_threshold + 1, padding=\"post\", truncating=\"post\")\n",
    "print(padded_seq_train.shape)\n",
    "\n",
    "#Pad testing data \n",
    "padded_seq_test = pad_sequences(sequences_test, maxlen=length_threshold + 1, padding=\"post\", truncating=\"post\")\n",
    "print(padded_seq_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Testing and Training Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.968217Z",
     "start_time": "2019-07-21T03:27:48.097Z"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary_train = t_train.word_index\n",
    "vocabulary_test = t_test.word_index \n",
    "\n",
    "\n",
    "vocabulary_inv_train = dict((v, k) for k, v in vocabulary_train.items())\n",
    "vocabulary_inv_test = dict((v, k) for k, v in vocabulary_test.items())\n",
    "vocabulary_inv_train[0] = \"<PAD/>\"\n",
    "vocabulary_inv_test[0] = \"<PAD/>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.969406Z",
     "start_time": "2019-07-21T03:27:48.398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2   2  11  11  56   2   2   2   2   2  11  11  14  29 137  29  29 137\n",
      "  29   1  13  28   1 170   1  53 247   1  64   1 171   1   1 171 115 115\n",
      "   4   4  12   4 126   1  27  26 216 216 201 201 216 129   3   9  97 130\n",
      "  14  14 108   8   1  40 215 140   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "(17869, 144)\n",
      "(132, 144)\n",
      "(17869,)\n",
      "(132,)\n"
     ]
    }
   ],
   "source": [
    "X_train = padded_seq_train \n",
    "X_test = padded_seq_test\n",
    "print(X_train[10, :])\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(filename):\n",
    "    \"\"\"\n",
    "    load embedding as python dictionary {root<str>: embeddings<np_array>}\n",
    "    :param filename: embedding.txt \n",
    "    :return: dictionary object mapping root to embeddings \n",
    "    \"\"\"\n",
    "    if not os.path.exists(filename): \n",
    "        print(\"please run 'Store Pre-Trained Embeddings Cell!'\")\n",
    "    else: \n",
    "        with open(filename, \"r\") as f: \n",
    "            lines = f.readlines()\n",
    "            f.close()\n",
    "            # create map of words to vectors \n",
    "            embedding = dict()\n",
    "            for line in lines: \n",
    "                comp = line.split()\n",
    "                # map of <str, numpy array> \n",
    "                embedding[comp[0]] = np.asarray(comp[1:], dtype='float32')\n",
    "            return embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_embed = load_embedding(\"embedding.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.979720Z",
     "start_time": "2019-07-21T03:27:50.237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train static shape: (17869, 144, 300)\n",
      "x_test static shape: (132, 144, 300)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.stack([np.stack([pre_embed [vocabulary_inv_train[action]] for action in commit]) for commit in X_train])\n",
    "X_test = np.stack([np.stack([pre_embed[vocabulary_inv_test[action]] for action in commit]) for commit in X_test])\n",
    "print(\"x_train static shape:\", X_train.shape)\n",
    "print(\"x_test static shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN for text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.982096Z",
     "start_time": "2019-07-21T03:27:50.555Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.985666Z",
     "start_time": "2019-07-21T03:27:50.725Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model type. See Kim Yoon's Convolutional Neural Networks for Sentence Classification, Section 3\n",
    "model_type = \"CNN-non-static\"  # CNN-rand|CNN-non-static|CNN-static\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 300\n",
    "filter_sizes = (5, 10)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.5)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 8\n",
    "num_epochs = 10 #50\n",
    "\n",
    "sequence_length = length_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.987127Z",
     "start_time": "2019-07-21T03:27:50.921Z"
    }
   },
   "outputs": [],
   "source": [
    "# input\n",
    "input_shape = (sequence_length + 1, embedding_dim)\n",
    "model_input = Input(shape=input_shape)\n",
    "\n",
    "z = model_input\n",
    "\n",
    "# dropout layer\n",
    "z = Dropout(dropout_prob[0])(z)\n",
    "\n",
    "# Convolutional block\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes: # Feature > Maintenance > Clean  up > Bug fix > \n",
    "    conv = Convolution1D(filters=num_filters,\n",
    "                         kernel_size=sz,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1)(z)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "\n",
    "z = Dropout(dropout_prob[1])(z)\n",
    "z = Dense(hidden_dims, activation=\"relu\")(z)\n",
    "\n",
    "model_output = Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer= optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True), \n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.987918Z",
     "start_time": "2019-07-21T03:27:51.138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17869 samples, validate on 132 samples\n",
      "Epoch 1/10\n",
      " - 22s - loss: 0.2032 - acc: 0.9168 - val_loss: 1.2427 - val_acc: 0.4848\n",
      "Epoch 2/10\n",
      " - 22s - loss: 0.0868 - acc: 0.9661 - val_loss: 1.4030 - val_acc: 0.5076\n",
      "Epoch 3/10\n",
      " - 21s - loss: 0.0666 - acc: 0.9711 - val_loss: 1.3696 - val_acc: 0.5076\n",
      "Epoch 4/10\n",
      " - 21s - loss: 0.0567 - acc: 0.9744 - val_loss: 1.4601 - val_acc: 0.4924\n",
      "Epoch 5/10\n",
      " - 21s - loss: 0.0488 - acc: 0.9788 - val_loss: 1.5317 - val_acc: 0.5152\n",
      "Epoch 6/10\n",
      " - 21s - loss: 0.0455 - acc: 0.9815 - val_loss: 1.6529 - val_acc: 0.4924\n",
      "Epoch 7/10\n",
      " - 21s - loss: 0.0446 - acc: 0.9813 - val_loss: 1.5653 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 21s - loss: 0.0414 - acc: 0.9820 - val_loss: 1.5183 - val_acc: 0.4924\n",
      "Epoch 9/10\n",
      " - 21s - loss: 0.0379 - acc: 0.9838 - val_loss: 1.6483 - val_acc: 0.5076\n",
      "Epoch 10/10\n",
      " - 21s - loss: 0.0367 - acc: 0.9835 - val_loss: 1.5959 - val_acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f15cbc2630>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.989615Z",
     "start_time": "2019-07-21T03:27:51.341Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred_bool = (y_pred > 0.5)\n",
    "\n",
    "predictions = y_pred_bool.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.991344Z",
     "start_time": "2019-07-21T03:27:51.561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.992898Z",
     "start_time": "2019-07-21T03:27:51.761Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def print_evaluation_scores(y_test, predicted):\n",
    "    \n",
    "    print('Accuracy:', accuracy_score(y_test, predicted))\n",
    "    print('F1-score macro:', f1_score(y_test, predicted, average='macro'))\n",
    "    print('F1-score micro:', f1_score(y_test, predicted, average='micro'))\n",
    "    print('F1-score weighted:', f1_score(y_test, predicted, average='weighted'))\n",
    "    print('Hamming_loss:', hamming_loss(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.994119Z",
     "start_time": "2019-07-21T03:27:52.337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "F1-score macro: 0.49954044117647056\n",
      "F1-score micro: 0.5\n",
      "F1-score weighted: 0.5011488970588235\n",
      "Hamming_loss: 0.5\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_scores(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

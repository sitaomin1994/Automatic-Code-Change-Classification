{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "import sys\n",
    "from collections import Counter \n",
    "import pprint \n",
    "import math\n",
    "import argparse \n",
    "import os\n",
    "import collections\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "import time \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(filepath):\n",
    "    \"\"\"\n",
    "    function used to parse json of each commit json file\n",
    "\n",
    "    Args:\n",
    "        filepath_list - list of filepaths\n",
    "\n",
    "    Returns:\n",
    "        files_json - list object contains parsed information\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    files_json = []\n",
    "    commit_ids = []\n",
    "\n",
    "    # each commits\n",
    "    files = os.listdir(filepath)\n",
    "    for path in files:\n",
    "        commit_id = path.split('_')[1].split('.')[0]\n",
    "        if os.stat(filepath + path).st_size != 0 and path != 'desktop.ini':\n",
    "            with open(filepath + path, encoding=\"utf8\") as f:\n",
    "                data = json.load(f)\n",
    "                files_list = []\n",
    "                # each file in commits\n",
    "                for file in data['files']:\n",
    "                    # parse only cluster file\n",
    "                    for key in file.keys():\n",
    "                        if re.match('^.*_cluster$', key):\n",
    "                            actions_list = []\n",
    "                            actions = file[key]['actions']\n",
    "                            # each action in file\n",
    "                            for action in actions:\n",
    "                                actions_list.append(action['root'])\n",
    "                            files_list.append(actions_list)\n",
    "            if len(files_list) != 0:\n",
    "                files_json.append(files_list)\n",
    "                commit_ids.append(commit_id)\n",
    "    assert len(commit_ids) == len(files_json)\n",
    "    # return\n",
    "    return files_json, commit_ids\n",
    "\n",
    "\n",
    "folder_path = './tmp_JSON_labeled_commits/'\n",
    "all_files, commit_ids = parse_json(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_roots(files_data):\n",
    "    counting = {}\n",
    "    for file_index, files in enumerate(files_data):\n",
    "        for root_index, roots in enumerate(files):\n",
    "            for action_index, actions in enumerate(roots):\n",
    "                temp = actions.split(' at ')[0].strip()\n",
    "                tempq = []\n",
    "                if temp.startswith('INS'):\n",
    "                    tempq.append('INS')\n",
    "                    words = [temp.split('INS ')[1].split('to ')[0].strip()\n",
    "                             ] + [temp.split('INS ')[1].split('to ')[-1].strip()]\n",
    "                    for items in words:\n",
    "\n",
    "                        items = items.split(': ')[0].strip()\n",
    "                        tempq.append(items)\n",
    "                    temp = '_'.join(tempq)\n",
    "\n",
    "                if temp.startswith('UPDATE'):\n",
    "                    temp = 'UPDATE'\n",
    "                if temp.startswith('MOVE'):\n",
    "                    temp2 = temp.split(' from ')[1].strip()\n",
    "                    tempq.append('MOVE')\n",
    "                    tempq.append(temp2.split(': ')[0].strip())\n",
    "                    temp = '_'.join(tempq)\n",
    "\n",
    "                if temp.startswith('DEL'):\n",
    "                    tempq.append('DEL')\n",
    "                    tempq.append(temp.split('DEL ')[1].split(': ')[0].strip())\n",
    "                    temp = '_'.join(tempq)\n",
    "                counting[temp] = counting.get(temp, 0) + 1\n",
    "                files_data[file_index][root_index][action_index] = temp\n",
    "    dic = {}\n",
    "    i = 0\n",
    "    for k, v in counting.items():\n",
    "        dic[k] = i  \n",
    "        i += 1\n",
    "    return dic, files_data, counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic, datas, freq_dict = preprocess_roots(all_files)\n",
    "rev_dic = dict(zip(dic.values(), dic.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actions2sentence(datas):\n",
    "    data_total = []\n",
    "    for files in datas:\n",
    "        data4file = []\n",
    "        for roots in files:\n",
    "            sentence = ' '.join(roots)\n",
    "            data4file.append(sentence)\n",
    "        data_total.append(data4file)\n",
    "    return data_total\n",
    "\n",
    "\n",
    "training_data = actions2sentence(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_files_to_sentence(training_data): \n",
    "    concat_data = \"\"\n",
    "    tmp_list = []\n",
    "    for items in training_data:\n",
    "        concat_data = \" \".join(items)\n",
    "        tmp_list.append(concat_data)\n",
    "        \n",
    "    return tmp_list\n",
    "\n",
    "concat_train_data = concat_files_to_sentence(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sample training data>:  ['INS_ImportDeclaration_CompilationUnit INS_FieldDeclaration_TypeDeclaration INS_FieldDeclaration_TypeDeclaration UPDATE MOVE_VariableDeclarationFragment INS_ExpressionStatement_Block INS_ExpressionStatement_Block UPDATE UPDATE INS_SimpleName_MethodInvocation INS_SimpleName_MethodInvocation INS_SimpleName_MethodInvocation INS_ClassInstanceCreation_MethodInvocation DEL_SimpleName DEL_ExpressionStatement DEL_SimpleName', 'INS_ImportDeclaration_CompilationUnit INS_VariableDeclarationStatement_Block INS_IfStatement_Block', 'INS_ImportDeclaration_CompilationUnit INS_FieldDeclaration_TypeDeclaration INS_SingleVariableDeclaration_MethodDeclaration INS_SingleVariableDeclaration_MethodDeclaration INS_ExpressionStatement_Block INS_ExpressionStatement_Block INS_ExpressionStatement_Block INS_ExpressionStatement_Block INS_ExpressionStatement_Block INS_ExpressionStatement_Block UPDATE INS_ExpressionStatement_Block INS_IfStatement_Block INS_ExpressionStatement_Block INS_ExpressionStatement_Block UPDATE INS_MethodInvocation_MethodInvocation INS_MethodInvocation_MethodInvocation', 'INS_ImportDeclaration_CompilationUnit INS_VariableDeclarationStatement_Block INS_IfStatement_Block', 'INS_ImportDeclaration_CompilationUnit INS_ImportDeclaration_CompilationUnit INS_MethodDeclaration_TypeDeclaration INS_MethodDeclaration_TypeDeclaration INS_IfStatement_Block INS_MethodInvocation_MethodInvocation', 'INS_ImportDeclaration_CompilationUnit INS_MethodDeclaration_TypeDeclaration INS_MethodDeclaration_TypeDeclaration INS_MethodDeclaration_TypeDeclaration INS_FieldDeclaration_TypeDeclaration INS_MethodDeclaration_TypeDeclaration INS_MethodDeclaration_TypeDeclaration MOVE_MethodDeclaration MOVE_MethodDeclaration INS_SingleVariableDeclaration_MethodDeclaration INS_Block_MethodDeclaration MOVE_MethodDeclaration INS_SingleVariableDeclaration_MethodDeclaration INS_SingleVariableDeclaration_MethodDeclaration MOVE_Block INS_ExpressionStatement_Block INS_MethodInvocation_ReturnStatement UPDATE MOVE_SimpleType UPDATE MOVE_SimpleType INS_SimpleName_SuperConstructorInvocation INS_SimpleName_SuperConstructorInvocation UPDATE MOVE_MethodInvocation MOVE_MethodInvocation INS_SimpleName_ClassInstanceCreation INS_SimpleName_ClassInstanceCreation DEL_ClassInstanceCreation DEL_MethodDeclaration DEL_MethodDeclaration']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_seqlength(training_data):\n",
    "    max_root_len = 0\n",
    "    seqlength_list = []\n",
    "    for item in training_data:\n",
    "        seqlength_list.append(len(item.split()))\n",
    "        if len(item.split()) >  max_root_len: \n",
    "            max_root_len = len(item.split())\n",
    "    return max_root_len, seqlength_list\n",
    "\n",
    "def plot_hist(seqlength_list): \n",
    "    plt.figure(figsize=(20,10))\n",
    "    number_of_files = np.array(seqlength_list)\n",
    "    bincount = np.bincount(seqlength_list)\n",
    "    x = np.arange(1, len(bincount)+1)\n",
    "    n, bins, patches = plt.hist(seqlength_list,x)\n",
    "\n",
    "max_seqlength, sequence_list = get_seqlength(concat_train_data)\n",
    "print(\"<sample training data>: \", training_data[0])\n",
    "plot_hist(sequence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275\n"
     ]
    }
   ],
   "source": [
    "# getting file threshold\n",
    "threshold = 0.95\n",
    "number_of_actions = [len(item.split()) for item in concat_train_data]\n",
    "\n",
    "def get_file_threshold(number_of_files, threshold = 0.95):\n",
    "    '''\n",
    "    get padding threshold for files dimension\n",
    "    \n",
    "    Args:\n",
    "        number_of_files - array of the number of files in each commits\n",
    "        threshold - drop all commits with its the number of files beyond this threshold\n",
    "    Returns:\n",
    "        padding threshold - number\n",
    "    '''\n",
    "    \n",
    "    total_files = len(number_of_files)\n",
    "    number_of_files = np.array(number_of_files)\n",
    "    bincount = np.bincount(number_of_files)\n",
    "\n",
    "    sum_file = 0\n",
    "    for index, item in enumerate(bincount):\n",
    "        sum_file += item\n",
    "        #print(index,item)\n",
    "        #print(sum_file)\n",
    "        if sum_file > threshold*total_files:\n",
    "            padding_files_threshold = index\n",
    "            break\n",
    "            \n",
    "    return padding_files_threshold\n",
    "\n",
    "length_threshold = get_file_threshold(number_of_actions, threshold)\n",
    "print(length_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csha</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3dd2210e79a8eb84378c370b32652f9a53f87a93</td>\n",
       "      <td>INS_ImportDeclaration_CompilationUnit INS_Fiel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e691b66aadbed87ac4891cec2ca5136bc85cfe4d</td>\n",
       "      <td>INS_VariableDeclarationStatement_Block INS_Try...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1f959d076ed7f29c3f8a5c6e99cbfcc62c1058d9</td>\n",
       "      <td>INS_ImportDeclaration_CompilationUnit INS_Impo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1a7286afce71c005bae8d45e6b280e977f823a79</td>\n",
       "      <td>INS_ImportDeclaration_CompilationUnit INS_Impo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4c0a65457cb7a16578592cfb2278a2bb99f78cad</td>\n",
       "      <td>MOVE_SingleVariableDeclaration DEL_TypeParamet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       csha  \\\n",
       "0  3dd2210e79a8eb84378c370b32652f9a53f87a93   \n",
       "1  e691b66aadbed87ac4891cec2ca5136bc85cfe4d   \n",
       "2  1f959d076ed7f29c3f8a5c6e99cbfcc62c1058d9   \n",
       "3  1a7286afce71c005bae8d45e6b280e977f823a79   \n",
       "4  4c0a65457cb7a16578592cfb2278a2bb99f78cad   \n",
       "\n",
       "                                            sequence  \n",
       "0  INS_ImportDeclaration_CompilationUnit INS_Fiel...  \n",
       "1  INS_VariableDeclarationStatement_Block INS_Try...  \n",
       "2  INS_ImportDeclaration_CompilationUnit INS_Impo...  \n",
       "3  INS_ImportDeclaration_CompilationUnit INS_Impo...  \n",
       "4  MOVE_SingleVariableDeclaration DEL_TypeParamet...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data = [ commit_ids, concat_train_data]).T\n",
    "df.columns = ['csha','sequence']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(774, 3)\n"
     ]
    }
   ],
   "source": [
    "df['len_seq'] = df.apply(lambda row: len(row['sequence'].split()), axis = 1)\n",
    "df = df[df['len_seq'] <= length_threshold].reset_index(drop = True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils.handle_labels import get_tag_counts_and_labels\n",
    "from utils.handle_labels import drop_labels\n",
    "from utils.handle_labels import group_labels\n",
    "from utils.handle_labels import categories_count\n",
    "from utils.handle_labels import get_imbalance\n",
    "from utils.handle_labels import label_distribution\n",
    "from utils.handle_labels import number_of_labels\n",
    "from utils.message_preprocess import message_processing\n",
    "# plot untils funcion\n",
    "from utils.plot_utils import pie_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1922, 28)\n",
      "<class 'list'>\n",
      "['Testing', 'Bug fix']\n"
     ]
    }
   ],
   "source": [
    "df_new = pd.read_csv('data/commit_data_new.csv')\n",
    "print(df_new.shape)\n",
    "\n",
    "# convert string to list\n",
    "from ast import literal_eval\n",
    "\n",
    "df_new['categories'] = df_new['categories'].apply(lambda x: literal_eval(x))\n",
    "print(type(df_new['categories'].values[0]))\n",
    "print(df_new['categories'].values[0])\n",
    "df_new = df_new.drop(['Unnamed: 0'], axis = 1)\n",
    "df_new = drop_labels(df_new, ['Testing', 'Build'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Commit ID</th>\n",
       "      <th>sequence</th>\n",
       "      <th>len_seq</th>\n",
       "      <th>project name</th>\n",
       "      <th>commit_message</th>\n",
       "      <th>Maintenance</th>\n",
       "      <th>Feature Add</th>\n",
       "      <th>Bug fix</th>\n",
       "      <th>Documentation</th>\n",
       "      <th>Clean up</th>\n",
       "      <th>...</th>\n",
       "      <th>Module Remove</th>\n",
       "      <th>Module Move</th>\n",
       "      <th>Rename</th>\n",
       "      <th>Versioning</th>\n",
       "      <th>Merge</th>\n",
       "      <th>Initialization</th>\n",
       "      <th>Internationalization</th>\n",
       "      <th>Data</th>\n",
       "      <th>Module Add</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3dd2210e79a8eb84378c370b32652f9a53f87a93</td>\n",
       "      <td>INS_ImportDeclaration_CompilationUnit INS_Fiel...</td>\n",
       "      <td>77</td>\n",
       "      <td>apache-parquet-mr</td>\n",
       "      <td>PARQUET-548: Add EncodingStats.\\n\\nThis adds `...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Feature Add]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e691b66aadbed87ac4891cec2ca5136bc85cfe4d</td>\n",
       "      <td>INS_VariableDeclarationStatement_Block INS_Try...</td>\n",
       "      <td>5</td>\n",
       "      <td>apache-santuario-java</td>\n",
       "      <td>Some test cleanup\\n\\ngit-svn-id: https://svn.a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Maintenance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1f959d076ed7f29c3f8a5c6e99cbfcc62c1058d9</td>\n",
       "      <td>INS_ImportDeclaration_CompilationUnit INS_Impo...</td>\n",
       "      <td>16</td>\n",
       "      <td>apache-giraph</td>\n",
       "      <td>GIRAPH-1168\\n\\ncloses #57\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Feature Add]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1a7286afce71c005bae8d45e6b280e977f823a79</td>\n",
       "      <td>INS_ImportDeclaration_CompilationUnit INS_Impo...</td>\n",
       "      <td>10</td>\n",
       "      <td>apache-hama</td>\n",
       "      <td>Remove hard-coded webapp path in HttpServer\\n\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Maintenance, Module Move]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4c0a65457cb7a16578592cfb2278a2bb99f78cad</td>\n",
       "      <td>MOVE_SingleVariableDeclaration DEL_TypeParamet...</td>\n",
       "      <td>6</td>\n",
       "      <td>google-truth</td>\n",
       "      <td>Remove unnecessary type parameter from about()...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Maintenance]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Commit ID  \\\n",
       "0  3dd2210e79a8eb84378c370b32652f9a53f87a93   \n",
       "1  e691b66aadbed87ac4891cec2ca5136bc85cfe4d   \n",
       "2  1f959d076ed7f29c3f8a5c6e99cbfcc62c1058d9   \n",
       "3  1a7286afce71c005bae8d45e6b280e977f823a79   \n",
       "4  4c0a65457cb7a16578592cfb2278a2bb99f78cad   \n",
       "\n",
       "                                            sequence  len_seq  \\\n",
       "0  INS_ImportDeclaration_CompilationUnit INS_Fiel...       77   \n",
       "1  INS_VariableDeclarationStatement_Block INS_Try...        5   \n",
       "2  INS_ImportDeclaration_CompilationUnit INS_Impo...       16   \n",
       "3  INS_ImportDeclaration_CompilationUnit INS_Impo...       10   \n",
       "4  MOVE_SingleVariableDeclaration DEL_TypeParamet...        6   \n",
       "\n",
       "            project name                                     commit_message  \\\n",
       "0      apache-parquet-mr  PARQUET-548: Add EncodingStats.\\n\\nThis adds `...   \n",
       "1  apache-santuario-java  Some test cleanup\\n\\ngit-svn-id: https://svn.a...   \n",
       "2          apache-giraph                        GIRAPH-1168\\n\\ncloses #57\\n   \n",
       "3            apache-hama  Remove hard-coded webapp path in HttpServer\\n\\...   \n",
       "4           google-truth  Remove unnecessary type parameter from about()...   \n",
       "\n",
       "   Maintenance  Feature Add  Bug fix  Documentation  Clean up  \\\n",
       "0            0            1        0              0         0   \n",
       "1            1            0        0              0         0   \n",
       "2            0            1        0              0         0   \n",
       "3            1            0        0              0         0   \n",
       "4            1            0        0              0         0   \n",
       "\n",
       "              ...              Module Remove  Module Move  Rename  Versioning  \\\n",
       "0             ...                          0            0       0           0   \n",
       "1             ...                          0            0       0           0   \n",
       "2             ...                          0            0       0           0   \n",
       "3             ...                          0            1       0           0   \n",
       "4             ...                          0            0       0           0   \n",
       "\n",
       "   Merge  Initialization  Internationalization  Data  Module Add  \\\n",
       "0      0               0                     0     0           0   \n",
       "1      0               0                     0     0           0   \n",
       "2      0               0                     0     0           0   \n",
       "3      0               0                     0     0           0   \n",
       "4      0               0                     0     0           0   \n",
       "\n",
       "                   categories  \n",
       "0               [Feature Add]  \n",
       "1               [Maintenance]  \n",
       "2               [Feature Add]  \n",
       "3  [Maintenance, Module Move]  \n",
       "4               [Maintenance]  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = ['Commit ID','sequence', 'len_seq']\n",
    "result = pd.merge(df, df_new, on='Commit ID')\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maintenance : 383\n",
      "Feature Add : 141\n",
      "Bug fix : 129\n",
      "Clean up : 89\n",
      "Documentation : 78\n",
      "Refactoring : 53\n",
      "Indentation : 29\n",
      "Token Replace : 16\n",
      "Cross : 12\n",
      "Legal : 10\n",
      "Debug : 8\n",
      "Source Control : 6\n",
      "Module Move : 3\n",
      "Module Remove : 3\n",
      "Rename : 2\n",
      "Versioning : 2\n",
      "Merge : 2\n",
      "Internationalization : 1\n",
      "Initialization : 1\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "_ , target_col = get_tag_counts_and_labels(result)\n",
    "output_dim = len(target_col)\n",
    "print(output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = ['Maintenance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = result[target_col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(765, 1)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 12, 12, 1, 34, 6, 6, 1, 1, 9, 9, 9, 98, 7, 16, 7, 2, 11, 17, 2, 12, 19, 19, 6, 6, 6, 6, 6, 6, 1, 6, 17, 6, 6, 1, 26, 26, 2, 11, 17, 2, 2, 4, 4, 17, 26, 2, 4, 4, 4, 12, 4, 4, 14, 14, 19, 36, 14, 19, 19, 8, 6, 68, 1, 18, 1, 18, 182, 182, 1, 5, 5, 53, 53, 56, 13, 13]\n"
     ]
    }
   ],
   "source": [
    "docs = result['sequence'].values\n",
    "t = Tokenizer(filters = '')\n",
    "t.fit_on_texts(docs)\n",
    "sequences = t.texts_to_sequences(docs)\n",
    "print(sequences[0])\n",
    "padded_seq = pad_sequences(sequences, maxlen=length_threshold, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = t.word_index\n",
    "vocabulary_inv = dict((v, k) for k, v in vocabulary.items())\n",
    "vocabulary_inv[0] = \"<PAD/>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(765, 275)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 0, shuffle = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(688, 275)\n",
      "(77, 275)\n",
      "(688, 1)\n",
      "(77, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from gensim.models import word2vec\n",
    "from os.path import join, exists, split\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(sentence_matrix, vocabulary_inv,\n",
    "                   num_features=300, min_word_count=1, context=10):\n",
    "    \"\"\"\n",
    "    Trains, saves, loads Word2Vec model\n",
    "    Returns initial weights for embedding layer.\n",
    "   \n",
    "    inputs:\n",
    "    sentence_matrix # int matrix: num_sentences x max_sentence_len\n",
    "    vocabulary_inv  # dict {int: str}\n",
    "    num_features    # Word vector dimensionality                      \n",
    "    min_word_count  # Minimum word count                        \n",
    "    context         # Context window size \n",
    "    \"\"\"\n",
    "    model_dir = 'models'\n",
    "    model_name = \"{:d}features_{:d}minwords_{:d}context\".format(num_features, min_word_count, context)\n",
    "    model_name = join(model_dir, model_name)\n",
    "    if exists(model_name):\n",
    "        embedding_model = word2vec.Word2Vec.load(model_name)\n",
    "        print('Load existing Word2Vec model \\'%s\\'' % split(model_name)[-1])\n",
    "    else:\n",
    "        # Set values for various parameters\n",
    "        num_workers = 2  # Number of threads to run in parallel\n",
    "        downsampling = 1e-3  # Downsample setting for frequent words\n",
    "\n",
    "        # Initialize and train the model\n",
    "        print('Training Word2Vec model...')\n",
    "        sentences = [[vocabulary_inv[w] for w in s] for s in sentence_matrix]\n",
    "        embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "                                            size=num_features, min_count=min_word_count,\n",
    "                                            window=context, sample=downsampling)\n",
    "\n",
    "        # If we don't plan to train the model any further, calling \n",
    "        # init_sims will make the model much more memory-efficient.\n",
    "        embedding_model.init_sims(replace=True)\n",
    "\n",
    "        # Saving the model for later use. You can load it later using Word2Vec.load()\n",
    "        if not exists(model_dir):\n",
    "            os.mkdir(model_dir)\n",
    "        print('Saving Word2Vec model \\'%s\\'' % split(model_name)[-1])\n",
    "        embedding_model.save(model_name)\n",
    "\n",
    "    # add unknown words\n",
    "    embedding_weights = {key: embedding_model[word] if word in embedding_model else\n",
    "                              np.random.uniform(-0.25, 0.25, embedding_model.vector_size)\n",
    "                         for key, word in vocabulary_inv.items()}\n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Saving Word2Vec model '300features_1minwords_5context'\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "min_word_count = 1\n",
    "context = 5\n",
    "embedding_weights = train_word2vec(padded_seq, vocabulary_inv, num_features=embedding_dim,\n",
    "                                       min_word_count=min_word_count, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train static shape: (688, 275, 300)\n",
      "x_test static shape: (77, 275, 300)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.stack([np.stack([embedding_weights[word] for word in sentence]) for sentence in X_train])\n",
    "X_test = np.stack([np.stack([embedding_weights[word] for word in sentence]) for sentence in X_test])\n",
    "print(\"x_train static shape:\", X_train.shape)\n",
    "print(\"x_test static shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN for text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model type. See Kim Yoon's Convolutional Neural Networks for Sentence Classification, Section 3\n",
    "model_type = \"CNN-non-static\"  # CNN-rand|CNN-non-static|CNN-static\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 300\n",
    "filter_sizes = (5, 10)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.5)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 8\n",
    "num_epochs = 50\n",
    "\n",
    "sequence_length = length_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_shape = (sequence_length, embedding_dim)\n",
    "model_input = Input(shape=input_shape)\n",
    "\n",
    "z = model_input\n",
    "\n",
    "# dropout layer\n",
    "z = Dropout(dropout_prob[0])(z)\n",
    "\n",
    "# Convolutional block\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Convolution1D(filters=num_filters,\n",
    "                         kernel_size=sz,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1)(z)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "\n",
    "z = Dropout(dropout_prob[1])(z)\n",
    "z = Dense(hidden_dims, activation=\"relu\")(z)\n",
    "\n",
    "model_output = Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer= optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True), \n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 688 samples, validate on 77 samples\n",
      "Epoch 1/50\n",
      " - 8s - loss: 0.6988 - acc: 0.4767 - val_loss: 0.6894 - val_acc: 0.5325\n",
      "Epoch 2/50\n",
      " - 6s - loss: 0.6918 - acc: 0.5203 - val_loss: 0.6855 - val_acc: 0.6234\n",
      "Epoch 3/50\n",
      " - 6s - loss: 0.6922 - acc: 0.5494 - val_loss: 0.6795 - val_acc: 0.6364\n",
      "Epoch 4/50\n",
      " - 6s - loss: 0.6849 - acc: 0.5378 - val_loss: 0.6744 - val_acc: 0.6234\n",
      "Epoch 5/50\n",
      " - 5s - loss: 0.6826 - acc: 0.5596 - val_loss: 0.6699 - val_acc: 0.6234\n",
      "Epoch 6/50\n",
      " - 5s - loss: 0.6828 - acc: 0.5625 - val_loss: 0.6669 - val_acc: 0.6494\n",
      "Epoch 7/50\n",
      " - 8s - loss: 0.6811 - acc: 0.5828 - val_loss: 0.6649 - val_acc: 0.6364\n",
      "Epoch 8/50\n",
      " - 9s - loss: 0.6836 - acc: 0.5654 - val_loss: 0.6636 - val_acc: 0.6234\n",
      "Epoch 9/50\n",
      " - 9s - loss: 0.6793 - acc: 0.5770 - val_loss: 0.6643 - val_acc: 0.6234\n",
      "Epoch 10/50\n",
      " - 6s - loss: 0.6705 - acc: 0.5843 - val_loss: 0.6624 - val_acc: 0.6234\n",
      "Epoch 11/50\n",
      " - 7s - loss: 0.6745 - acc: 0.6017 - val_loss: 0.6616 - val_acc: 0.6364\n",
      "Epoch 12/50\n",
      " - 12s - loss: 0.6702 - acc: 0.5828 - val_loss: 0.6610 - val_acc: 0.6494\n",
      "Epoch 13/50\n",
      " - 7s - loss: 0.6750 - acc: 0.5785 - val_loss: 0.6601 - val_acc: 0.6364\n",
      "Epoch 14/50\n",
      " - 5s - loss: 0.6694 - acc: 0.5916 - val_loss: 0.6611 - val_acc: 0.6234\n",
      "Epoch 15/50\n",
      " - 6s - loss: 0.6725 - acc: 0.5756 - val_loss: 0.6628 - val_acc: 0.6234\n",
      "Epoch 16/50\n",
      " - 7s - loss: 0.6726 - acc: 0.5945 - val_loss: 0.6588 - val_acc: 0.6234\n",
      "Epoch 17/50\n",
      " - 6s - loss: 0.6650 - acc: 0.6061 - val_loss: 0.6591 - val_acc: 0.6364\n",
      "Epoch 18/50\n",
      " - 6s - loss: 0.6609 - acc: 0.6032 - val_loss: 0.6610 - val_acc: 0.6234\n",
      "Epoch 19/50\n",
      " - 6s - loss: 0.6719 - acc: 0.5770 - val_loss: 0.6656 - val_acc: 0.6104\n",
      "Epoch 20/50\n",
      " - 6s - loss: 0.6595 - acc: 0.6090 - val_loss: 0.6635 - val_acc: 0.5974\n",
      "Epoch 21/50\n",
      " - 6s - loss: 0.6707 - acc: 0.5901 - val_loss: 0.6642 - val_acc: 0.5974\n",
      "Epoch 22/50\n",
      " - 6s - loss: 0.6628 - acc: 0.6003 - val_loss: 0.6626 - val_acc: 0.6104\n",
      "Epoch 23/50\n",
      " - 6s - loss: 0.6576 - acc: 0.6047 - val_loss: 0.6641 - val_acc: 0.6104\n",
      "Epoch 24/50\n",
      " - 6s - loss: 0.6557 - acc: 0.6192 - val_loss: 0.6630 - val_acc: 0.6104\n",
      "Epoch 25/50\n",
      " - 6s - loss: 0.6537 - acc: 0.6250 - val_loss: 0.6619 - val_acc: 0.6234\n",
      "Epoch 26/50\n",
      " - 6s - loss: 0.6568 - acc: 0.6163 - val_loss: 0.6664 - val_acc: 0.6234\n",
      "Epoch 27/50\n",
      " - 6s - loss: 0.6543 - acc: 0.6032 - val_loss: 0.6660 - val_acc: 0.5974\n",
      "Epoch 28/50\n",
      " - 6s - loss: 0.6571 - acc: 0.6134 - val_loss: 0.6642 - val_acc: 0.6104\n",
      "Epoch 29/50\n",
      " - 8s - loss: 0.6485 - acc: 0.6163 - val_loss: 0.6670 - val_acc: 0.6104\n",
      "Epoch 30/50\n",
      " - 8s - loss: 0.6558 - acc: 0.6061 - val_loss: 0.6701 - val_acc: 0.5974\n",
      "Epoch 31/50\n",
      " - 7s - loss: 0.6521 - acc: 0.6163 - val_loss: 0.6687 - val_acc: 0.5974\n",
      "Epoch 32/50\n",
      " - 7s - loss: 0.6479 - acc: 0.6148 - val_loss: 0.6725 - val_acc: 0.5974\n",
      "Epoch 33/50\n",
      " - 7s - loss: 0.6453 - acc: 0.6221 - val_loss: 0.6723 - val_acc: 0.5974\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-8a7e670e1fd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs,\n\u001b[0;32m----> 2\u001b[0;31m           validation_data=(X_test, y_test), verbose=2)\n\u001b[0m",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred_bool = (y_pred > 0.5)\n",
    "\n",
    "predictions = y_pred_bool.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def print_evaluation_scores(y_test, predicted):\n",
    "    \n",
    "    print('Accuracy:', accuracy_score(y_test, predicted))\n",
    "    print('F1-score macro:', f1_score(y_test, predicted, average='macro'))\n",
    "    print('F1-score micro:', f1_score(y_test, predicted, average='micro'))\n",
    "    print('F1-score weighted:', f1_score(y_test, predicted, average='weighted'))\n",
    "    print('Hamming_loss:', hamming_loss(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6363636363636364\n",
      "F1-score macro: 0.6363022941970311\n",
      "F1-score micro: 0.6363636363636364\n",
      "F1-score weighted: 0.6364863206968471\n",
      "Hamming_loss: 0.36363636363636365\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_scores(y_test, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:43.628313Z",
     "start_time": "2019-07-21T03:27:34.819524Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "import sys\n",
    "from collections import Counter \n",
    "import pprint \n",
    "import math\n",
    "import argparse \n",
    "import os\n",
    "import collections\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "import time \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:46.300541Z",
     "start_time": "2019-07-21T03:27:43.631478Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_json(filepath):\n",
    "    \"\"\"\n",
    "    function used to parse json of each commit json file\n",
    "\n",
    "    Args:\n",
    "        filepath_list - list of filepaths\n",
    "\n",
    "    Returns:\n",
    "        files_json - list object contains parsed information\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    files_json = []\n",
    "    commit_ids = []\n",
    "    # each commits\n",
    "    files = os.listdir(filepath)\n",
    "    for path in files:\n",
    "        commit_id = path.split(\"_\")[1].split(\".\")[0]\n",
    "        if os.stat(filepath + path).st_size != 0 and path != 'desktop.ini':\n",
    "            with open(filepath + path, encoding=\"utf8\") as f:\n",
    "                data = json.load(f)\n",
    "                files_list = []\n",
    "                # each file in commits\n",
    "                for file in data['files']:\n",
    "                    # parse only cluster file\n",
    "                    for key in file.keys():\n",
    "                        if re.match('^.*_cluster$', key):\n",
    "                            actions_list = []\n",
    "                            actions = file[key]['actions']\n",
    "                            # each action in file\n",
    "                            for action in actions:\n",
    "                                actions_list.append(action['root'])\n",
    "                            files_list.append(actions_list)\n",
    "            if len(files_list) != 0:\n",
    "                files_json.append(files_list)\n",
    "                commit_ids.append(commit_id)\n",
    "    assert(len(commit_ids) == len(files_json))      \n",
    "    # return\n",
    "    return files_json, commit_ids\n",
    "\n",
    "\n",
    "folder_path = 'C:\\\\Users\\\\ichel\\\\Desktop\\\\tmp_JSON_labeled_commits\\\\'\n",
    "all_files, csha = parse_json(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:46.324762Z",
     "start_time": "2019-07-21T03:27:46.305599Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_roots(files_data):\n",
    "    counting = {}\n",
    "    for file_index, files in enumerate(files_data):\n",
    "        for root_index, roots in enumerate(files):\n",
    "            for action_index, actions in enumerate(roots):\n",
    "                temp = actions.split(' at ')[0].strip()\n",
    "                tempq = []\n",
    "                if temp.startswith('INS'):\n",
    "                    tempq.append('INS')\n",
    "                    words = [temp.split('INS ')[1].split('to ')[0].strip()] + [\n",
    "                        temp.split('INS ')[1].rsplit('to ')[-1].strip()\n",
    "                    ]\n",
    "                    for items in words:\n",
    "                        items = items.split(':')[0].strip()\n",
    "                        tempq.append(items)\n",
    "                    if tempq[1] == 'TextElement' and tempq[-1] not in ['TagElement', 'TextElement']:\n",
    "                        tempq[-1] = ''\n",
    "                    temp = '_'.join(tempq)\n",
    "\n",
    "                if temp.startswith('UPDATE'):\n",
    "                    temp = 'UPDATE'\n",
    "                if temp.startswith('MOVE'):\n",
    "                    temp2 = temp.split('from ')[1].strip()\n",
    "                    tempq.append('MOVE')\n",
    "                    tempq.append(temp2.split(':')[0].strip())\n",
    "                    temp = '_'.join(tempq)\n",
    "\n",
    "                if temp.startswith('DEL'):\n",
    "                    tempq.append('DEL')\n",
    "                    tempq.append(temp.split('DEL ')[1].split(':')[0].strip())\n",
    "                    temp = '_'.join(tempq)\n",
    "                temp = temp.replace(' ', '_')\n",
    "                counting[temp] = counting.get(temp, 0) + 1\n",
    "                files_data[file_index][root_index][action_index] = temp\n",
    "    dic = {}\n",
    "    i = 0\n",
    "    for k, v in counting.items():\n",
    "        dic[k] = i\n",
    "        i += 1\n",
    "    return dic, files_data, counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:46.475539Z",
     "start_time": "2019-07-21T03:27:46.328083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n"
     ]
    }
   ],
   "source": [
    "dic, datas, freq_dict = preprocess_roots(all_files)\n",
    "rev_dic = dict(zip(dic.values(), dic.keys()))\n",
    "print(len(dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:46.488683Z",
     "start_time": "2019-07-21T03:27:46.478368Z"
    }
   },
   "outputs": [],
   "source": [
    "def actions2sentence(datas):\n",
    "    data_total = []\n",
    "    for files in datas:\n",
    "        data4file = []\n",
    "        for roots in files:\n",
    "            sentence = ' '.join(roots)\n",
    "            data4file.append(sentence)\n",
    "        data_total.append(data4file)\n",
    "    return data_total\n",
    "\n",
    "\n",
    "training_data = actions2sentence(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_index_strange_words(training_data):\n",
    "    for i, item in enumerate(training_data):\n",
    "        for j, file in enumerate(item):\n",
    "            for k, action in enumerate(file.split()):\n",
    "                index = dic.get(action)\n",
    "                if index is None: # for debug \n",
    "                    print(\"==============================\")\n",
    "                    print(action) # for debug \n",
    "                    print('commits index: %d'%i)\n",
    "                    print('file: %d'%j)\n",
    "                    print('action: %d'%k)\n",
    "check_index_strange_words(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permute Order of File \n",
    "**Purpose**: Here we permutate the order of files within a given commit while still maintaining the labels for that commit. \n",
    "This helps our CNN by: \n",
    "- 1) Increasing the training samples \n",
    "- 2) Make the CNN invariant to file location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop labels\n",
    "def drop_labels(df, labels):\n",
    "    \"\"\"\n",
    "    Drop some of labels\n",
    "\n",
    "    Args:\n",
    "    df - Dataframs\n",
    "    labels - List of labels name to drop\n",
    "\n",
    "    Returns:\n",
    "    new_df -  new dataframe\n",
    "    \"\"\"\n",
    "    # remove labels in categories list\n",
    "    new_df = df.copy()\n",
    "    new_df['categories'] = new_df['categories'].apply(lambda row: [item for item in row if item not in labels])\n",
    "\n",
    "    # remove columns\n",
    "    new_df = new_df.drop(labels, axis=1)\n",
    "\n",
    "    # remove columns which have no labels after removing labels\n",
    "    new_df['number_of_labels'] = new_df['categories'].apply(lambda row: len(row))\n",
    "    new_df = new_df[new_df['number_of_labels'] != 0].reset_index(drop=True)\n",
    "    new_df = new_df.drop(['number_of_labels'], axis=1)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group some of labels\n",
    "def group_labels(df, labels_to_group, new_label):\n",
    "    '''\n",
    "    Group some of labels\n",
    "\n",
    "    Args:\n",
    "        df - dataframe\n",
    "        labels_to_group -  List of labels you want to group\n",
    "        new_label -  string - new label name of grouped labels\n",
    "\n",
    "    Returns:\n",
    "        new_df - dataframe after grouped\n",
    "    '''\n",
    "    new_df = df.copy()\n",
    "\n",
    "    # generate new labels by group labels\n",
    "    def create_new_label(row, labels):\n",
    "        new_label = 0  # initialize new label\n",
    "        for label in labels:\n",
    "            if row[label] == 1:\n",
    "                new_label = 1  # if one of labels in grouped labels is 1 the new label is 1\n",
    "        return new_label\n",
    "\n",
    "    new_df[new_label] = df.apply(lambda row: create_new_label(row, labels_to_group), axis=1)\n",
    "\n",
    "    # drop old labels\n",
    "    new_df = new_df.drop(labels_to_group, axis=1)\n",
    "    \n",
    "    # generate list of new_categories\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation: \n",
    "Prepare data for embedding and training ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1922, 28)\n",
      "<class 'list'>\n",
      "['Testing', 'Bug fix']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Commit ID', 'project name', 'commit_message', 'Maintenance',\n",
       "       'Feature Add', 'Bug fix', 'Clean up', 'Refactoring', 'Token Replace',\n",
       "       'categories', 'Files', 'Cross_', 'Documentation_'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "# new = list(itertools.permutations([1, 2, 3]))   0: perm1, perm2, permutation3 \n",
    "# list_new = []\n",
    "# for ii in new:\n",
    "#     list_new.append(list(ii))\n",
    "    \n",
    "# for ii in list_new: \n",
    "#     print(type(ii))\n",
    "\n",
    "commits_dic = dict()\n",
    "for sha, training_file in zip(csha, training_data): \n",
    "    commits_dic[sha] = []\n",
    "    if len(training_file) <= 6: \n",
    "        tmp_permutate = list(itertools.permutations(training_file))\n",
    "        for permutated_file in tmp_permutate: \n",
    "            commits_dic[sha].append(list(permutated_file))\n",
    "    else: \n",
    "        commits_dic[sha].append(training_file)\n",
    "commits_df = pd.DataFrame(commits_dic.items())\n",
    "commits_df.columns = [\"Commit ID\", \"Files\"]\n",
    "\n",
    "df_new = pd.read_csv('C:\\\\Users\\\\ichel\\\\Desktop\\\\commit_data_new.csv')\n",
    "print(df_new.shape)\n",
    "\n",
    "# convert string to list\n",
    "from ast import literal_eval\n",
    "\n",
    "df_new['categories'] = df_new['categories'].apply(lambda x: literal_eval(x))\n",
    "print(type(df_new['categories'].values[0]))\n",
    "print(df_new['categories'].values[0])\n",
    "df_new = df_new.drop(['Unnamed: 0'], axis = 1)\n",
    "commits_labels_df = pd.merge(commits_df, df_new, on='Commit ID')\n",
    "commits_labels_df.head(1)\n",
    "s= commits_labels_df.apply(lambda x: pd.Series(x['Files']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "s.name = \"Files\"\n",
    "commits_labels_df = commits_labels_df.drop(\"Files\", axis=1) \n",
    "commits_labels_df = commits_labels_df.join(s)\n",
    "commits_labels_df = drop_labels(commits_labels_df, [\"Testing\", \"Build\", \"Versioning\", \"Indentation\", \"Internationalization\", \"Merge\", \\\n",
    "                                                   \"Module Move\", \"Module Remove\", \"Source Control\", \"Rename\", \"Initialization\", \\\n",
    "                                                   \"Module Add\", \"Data\"])\n",
    "commits_labels_df = group_labels(commits_labels_df, [\"Cross\", \"Debug\"], \"Cross_\")\n",
    "commits_labels_df = group_labels(commits_labels_df, [\"Legal\", \"Documentation\"], \"Documentation_\")\n",
    "commits_labels_df.shape\n",
    "commit_files = commits_labels_df[\"Files\"].values\n",
    "commits_labels_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:46.502749Z",
     "start_time": "2019-07-21T03:27:46.491433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOVE_CompilationUnit UPDATE INS_MethodDeclaration_TypeDeclaration INS_SimpleType_MethodDeclaration INS_SingleVariableDeclaration_MethodDeclaration INS_SingleVariableDeclaration_MethodDeclaration INS_SimpleType_MethodDeclaration INS_Block_MethodDeclaration MOVE_MethodDeclaration INS_VariableDeclarationStatement_Block INS_MethodInvocation_ExpressionStatement MOVE_ConditionalExpression MOVE_MethodInvocation UPDATE DEL_ImportDeclaration DEL_SimpleName\n"
     ]
    }
   ],
   "source": [
    "def concat_files_to_sentence(training_data): \n",
    "    concat_data = \"\"\n",
    "    tmp_list = []\n",
    "    for items in training_data:\n",
    "        concat_data = \" \".join(items)\n",
    "        tmp_list.append(concat_data)\n",
    "        \n",
    "    return tmp_list\n",
    "concat_train_data = concat_files_to_sentence(commit_files)\n",
    "print(concat_train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine File Threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.112668Z",
     "start_time": "2019-07-21T03:27:46.510367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sample training data>:  ['UPDATE UPDATE']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_seqlength(training_data):\n",
    "    max_root_len = 0\n",
    "    seqlength_list = []\n",
    "    for item in training_data:\n",
    "        seqlength_list.append(len(item.split()))\n",
    "        if len(item.split()) >  max_root_len: \n",
    "            max_root_len = len(item.split())\n",
    "    return max_root_len, seqlength_list\n",
    "\n",
    "def plot_hist(seqlength_list): \n",
    "    plt.figure(figsize=(20,10))\n",
    "    number_of_files = np.array(seqlength_list)\n",
    "    bincount = np.bincount(seqlength_list)\n",
    "    x = np.arange(1, len(bincount)+1)\n",
    "    n, bins, patches = plt.hist(seqlength_list,x)\n",
    "    plt.xlim((0, 500))\n",
    "    plt.ylim((0, 2000))\n",
    "\n",
    "max_seqlength, sequence_list = get_seqlength(concat_train_data)\n",
    "print(\"<sample training data>: \", training_data[0])\n",
    "plot_hist(sequence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.136546Z",
     "start_time": "2019-07-21T03:27:53.117407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n"
     ]
    }
   ],
   "source": [
    "# getting file threshold\n",
    "threshold = 0.95\n",
    "number_of_actions = [len(item.split()) for item in concat_train_data]\n",
    "\n",
    "def get_file_threshold(number_of_files, threshold = 0.95):\n",
    "    '''\n",
    "    get padding threshold for files dimension\n",
    "    \n",
    "    Args:\n",
    "        number_of_files - array of the number of files in each commits\n",
    "        threshold - drop all commits with its the number of files beyond this threshold\n",
    "    Returns:\n",
    "        padding threshold - number\n",
    "    '''\n",
    "    \n",
    "    total_files = len(number_of_files)\n",
    "    number_of_files = np.array(number_of_files)\n",
    "    bincount = np.bincount(number_of_files)\n",
    "\n",
    "    sum_file = 0\n",
    "    for index, item in enumerate(bincount):\n",
    "        sum_file += item\n",
    "        #print(index,item)\n",
    "        #print(sum_file)\n",
    "        if sum_file > threshold*total_files:\n",
    "            padding_files_threshold = index\n",
    "            break\n",
    "            \n",
    "    return padding_files_threshold\n",
    "\n",
    "length_threshold = get_file_threshold(number_of_actions, threshold)\n",
    "print(length_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "commits_labels_df[\"Files\"] = concat_train_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.530703Z",
     "start_time": "2019-07-21T03:27:53.490766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Commit ID', 'project name', 'commit_message', 'Maintenance',\n",
      "       'Feature Add', 'Bug fix', 'Clean up', 'Refactoring', 'Token Replace',\n",
      "       'categories', 'Files', 'Cross_', 'Documentation_', 'len_seq'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "commits_labels_df['len_seq'] = commits_labels_df.apply(lambda row: len(row['Files'].split()), axis = 1)\n",
    "commits_labels_df = commits_labels_df[commits_labels_df['len_seq'] <= length_threshold].reset_index(drop = True)\n",
    "print(commits_labels_df.columns)  #### (21128, 21) :Here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.960319Z",
     "start_time": "2019-07-21T03:27:47.213Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21121, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_col = [\"Maintenance\", \"Feature Add\", \"Bug fix\", \"Clean up\", \"Refactoring\", \"Token Replace\", \"Cross_\", \"Documentation_\"]\n",
    "y = commits_labels_df[target_col].values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Pad data \n",
    "We tokenize the data and pad with the token <PAD/>.<br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.966249Z",
     "start_time": "2019-07-21T03:27:47.781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 1, 3, 143, 13, 13, 143, 43, 9, 14, 109, 102, 6, 1, 4, 8]\n"
     ]
    }
   ],
   "source": [
    "docs = commits_labels_df['Files'].values\n",
    "t = Tokenizer(filters = '', lower=False)\n",
    "t.fit_on_texts(docs)\n",
    "sequences = t.texts_to_sequences(docs)\n",
    "print(sequences[0])\n",
    "padded_seq = pad_sequences(sequences, maxlen=length_threshold + 1, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.968217Z",
     "start_time": "2019-07-21T03:27:48.097Z"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary = t.word_index\n",
    "vocabulary_inv = dict((v, k) for k, v in vocabulary.items())\n",
    "vocabulary_inv[0] = \"<PAD/>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.969406Z",
     "start_time": "2019-07-21T03:27:48.398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2   2  11   3   3 293 215   7  14  14  63  95 125   1   5 157 133   7\n",
      "  49 109 109   1   6  49  22  22 150  17  17  62   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(21121, 144)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = padded_seq\n",
    "print(X[2, :])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Testing and Training Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.970717Z",
     "start_time": "2019-07-21T03:27:48.859Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 0, shuffle = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.972439Z",
     "start_time": "2019-07-21T03:27:49.033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19008, 144)\n",
      "(2113, 144)\n",
      "(19008, 8)\n",
      "(2113, 8)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(filename):\n",
    "    \"\"\"\n",
    "    load embedding as python dictionary {root<str>: embeddings<np_array>}\n",
    "    :param filename: embedding.txt \n",
    "    :return: dictionary object mapping root to embeddings \n",
    "    \"\"\"\n",
    "    if not os.path.exists(filename): \n",
    "        print(\"please run 'Store Pre-Trained Embeddings Cell!'\")\n",
    "    else: \n",
    "        with open(filename, \"r\") as f: \n",
    "            lines = f.readlines()\n",
    "            f.close()\n",
    "            # create map of words to vectors \n",
    "            embedding = dict()\n",
    "            for line in lines: \n",
    "                comp = line.split()\n",
    "                # map of <str, numpy array> \n",
    "                embedding[comp[0]] = np.asarray(comp[1:], dtype='float32')\n",
    "            return embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_embed = load_embedding(\"embedding.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.979720Z",
     "start_time": "2019-07-21T03:27:50.237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train static shape: (19008, 144, 300)\n",
      "x_test static shape: (2113, 144, 300)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.stack([np.stack([pre_embed [vocabulary_inv[action]] for action in commit]) for commit in X_train])\n",
    "X_test = np.stack([np.stack([pre_embed[vocabulary_inv[action]] for action in commit]) for commit in X_test])\n",
    "print(\"x_train static shape:\", X_train.shape)\n",
    "print(\"x_test static shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN for text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.982096Z",
     "start_time": "2019-07-21T03:27:50.555Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.985666Z",
     "start_time": "2019-07-21T03:27:50.725Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model type. See Kim Yoon's Convolutional Neural Networks for Sentence Classification, Section 3\n",
    "model_type = \"CNN-non-static\"  # CNN-rand|CNN-non-static|CNN-static\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 300\n",
    "filter_sizes = (5, 10)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.5)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 8\n",
    "num_epochs = 50\n",
    "\n",
    "sequence_length = length_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.987127Z",
     "start_time": "2019-07-21T03:27:50.921Z"
    }
   },
   "outputs": [],
   "source": [
    "# input\n",
    "input_shape = (sequence_length + 1, embedding_dim)\n",
    "model_input = Input(shape=input_shape)\n",
    "\n",
    "z = model_input\n",
    "\n",
    "# dropout layer\n",
    "z = Dropout(dropout_prob[0])(z)\n",
    "\n",
    "# Convolutional block\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Convolution1D(filters=num_filters,\n",
    "                         kernel_size=sz,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1)(z)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "\n",
    "z = Dropout(dropout_prob[1])(z)\n",
    "z = Dense(hidden_dims, activation=\"relu\")(z)\n",
    "\n",
    "model_output = Dense(8, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer= optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True), \n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.987918Z",
     "start_time": "2019-07-21T03:27:51.138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19008 samples, validate on 2113 samples\n",
      "Epoch 1/50\n",
      " - 25s - loss: 0.2903 - acc: 0.8777 - val_loss: 0.1710 - val_acc: 0.9426\n",
      "Epoch 2/50\n",
      " - 22s - loss: 0.1515 - acc: 0.9425 - val_loss: 0.0961 - val_acc: 0.9724\n",
      "Epoch 3/50\n",
      " - 21s - loss: 0.1001 - acc: 0.9677 - val_loss: 0.0676 - val_acc: 0.9815\n",
      "Epoch 4/50\n",
      " - 22s - loss: 0.0798 - acc: 0.9754 - val_loss: 0.0545 - val_acc: 0.9846\n",
      "Epoch 5/50\n",
      " - 22s - loss: 0.0677 - acc: 0.9797 - val_loss: 0.0484 - val_acc: 0.9856\n",
      "Epoch 6/50\n",
      " - 22s - loss: 0.0614 - acc: 0.9813 - val_loss: 0.0436 - val_acc: 0.9872\n",
      "Epoch 7/50\n",
      " - 22s - loss: 0.0553 - acc: 0.9835 - val_loss: 0.0403 - val_acc: 0.9879\n",
      "Epoch 8/50\n",
      " - 22s - loss: 0.0521 - acc: 0.9845 - val_loss: 0.0381 - val_acc: 0.9892\n",
      "Epoch 9/50\n",
      " - 22s - loss: 0.0482 - acc: 0.9858 - val_loss: 0.0365 - val_acc: 0.9887\n",
      "Epoch 10/50\n",
      " - 22s - loss: 0.0458 - acc: 0.9863 - val_loss: 0.0352 - val_acc: 0.9895\n",
      "Epoch 11/50\n",
      " - 22s - loss: 0.0443 - acc: 0.9864 - val_loss: 0.0339 - val_acc: 0.9893\n",
      "Epoch 12/50\n",
      " - 22s - loss: 0.0426 - acc: 0.9871 - val_loss: 0.0327 - val_acc: 0.9897\n",
      "Epoch 13/50\n",
      " - 21s - loss: 0.0415 - acc: 0.9876 - val_loss: 0.0315 - val_acc: 0.9902\n",
      "Epoch 14/50\n",
      " - 22s - loss: 0.0399 - acc: 0.9878 - val_loss: 0.0310 - val_acc: 0.9902\n",
      "Epoch 15/50\n",
      " - 22s - loss: 0.0388 - acc: 0.9882 - val_loss: 0.0304 - val_acc: 0.9902\n",
      "Epoch 16/50\n",
      " - 22s - loss: 0.0370 - acc: 0.9886 - val_loss: 0.0299 - val_acc: 0.9903\n",
      "Epoch 17/50\n",
      " - 21s - loss: 0.0366 - acc: 0.9887 - val_loss: 0.0292 - val_acc: 0.9908\n",
      "Epoch 18/50\n",
      " - 21s - loss: 0.0347 - acc: 0.9890 - val_loss: 0.0284 - val_acc: 0.9911\n",
      "Epoch 19/50\n",
      " - 21s - loss: 0.0346 - acc: 0.9891 - val_loss: 0.0285 - val_acc: 0.9911\n",
      "Epoch 20/50\n",
      " - 21s - loss: 0.0336 - acc: 0.9894 - val_loss: 0.0279 - val_acc: 0.9912\n",
      "Epoch 21/50\n",
      " - 21s - loss: 0.0329 - acc: 0.9898 - val_loss: 0.0275 - val_acc: 0.9917\n",
      "Epoch 22/50\n",
      " - 21s - loss: 0.0322 - acc: 0.9900 - val_loss: 0.0271 - val_acc: 0.9917\n",
      "Epoch 23/50\n",
      " - 21s - loss: 0.0318 - acc: 0.9903 - val_loss: 0.0266 - val_acc: 0.9914\n",
      "Epoch 24/50\n",
      " - 21s - loss: 0.0313 - acc: 0.9901 - val_loss: 0.0264 - val_acc: 0.9920\n",
      "Epoch 25/50\n",
      " - 21s - loss: 0.0310 - acc: 0.9902 - val_loss: 0.0262 - val_acc: 0.9920\n",
      "Epoch 26/50\n",
      " - 21s - loss: 0.0306 - acc: 0.9905 - val_loss: 0.0259 - val_acc: 0.9921\n",
      "Epoch 27/50\n",
      " - 21s - loss: 0.0298 - acc: 0.9905 - val_loss: 0.0256 - val_acc: 0.9925\n",
      "Epoch 28/50\n",
      " - 21s - loss: 0.0292 - acc: 0.9907 - val_loss: 0.0252 - val_acc: 0.9922\n",
      "Epoch 29/50\n",
      " - 21s - loss: 0.0291 - acc: 0.9909 - val_loss: 0.0251 - val_acc: 0.9923\n",
      "Epoch 30/50\n",
      " - 21s - loss: 0.0289 - acc: 0.9907 - val_loss: 0.0249 - val_acc: 0.9920\n",
      "Epoch 31/50\n",
      " - 21s - loss: 0.0289 - acc: 0.9909 - val_loss: 0.0247 - val_acc: 0.9923\n",
      "Epoch 32/50\n",
      " - 21s - loss: 0.0273 - acc: 0.9915 - val_loss: 0.0245 - val_acc: 0.9924\n",
      "Epoch 33/50\n",
      " - 21s - loss: 0.0271 - acc: 0.9914 - val_loss: 0.0242 - val_acc: 0.9926\n",
      "Epoch 34/50\n",
      " - 21s - loss: 0.0274 - acc: 0.9913 - val_loss: 0.0240 - val_acc: 0.9925\n",
      "Epoch 35/50\n",
      " - 21s - loss: 0.0269 - acc: 0.9913 - val_loss: 0.0238 - val_acc: 0.9926\n",
      "Epoch 36/50\n",
      " - 21s - loss: 0.0267 - acc: 0.9913 - val_loss: 0.0239 - val_acc: 0.9928\n",
      "Epoch 37/50\n",
      " - 21s - loss: 0.0264 - acc: 0.9913 - val_loss: 0.0236 - val_acc: 0.9929\n",
      "Epoch 38/50\n",
      " - 21s - loss: 0.0264 - acc: 0.9917 - val_loss: 0.0234 - val_acc: 0.9928\n",
      "Epoch 39/50\n",
      " - 21s - loss: 0.0254 - acc: 0.9918 - val_loss: 0.0233 - val_acc: 0.9930\n",
      "Epoch 40/50\n",
      " - 21s - loss: 0.0252 - acc: 0.9920 - val_loss: 0.0231 - val_acc: 0.9928\n",
      "Epoch 41/50\n",
      " - 21s - loss: 0.0248 - acc: 0.9919 - val_loss: 0.0229 - val_acc: 0.9928\n",
      "Epoch 42/50\n",
      " - 21s - loss: 0.0251 - acc: 0.9919 - val_loss: 0.0231 - val_acc: 0.9930\n",
      "Epoch 43/50\n",
      " - 22s - loss: 0.0243 - acc: 0.9920 - val_loss: 0.0227 - val_acc: 0.9929\n",
      "Epoch 44/50\n",
      " - 21s - loss: 0.0247 - acc: 0.9920 - val_loss: 0.0226 - val_acc: 0.9929\n",
      "Epoch 45/50\n",
      " - 21s - loss: 0.0241 - acc: 0.9921 - val_loss: 0.0225 - val_acc: 0.9930\n",
      "Epoch 46/50\n",
      " - 21s - loss: 0.0240 - acc: 0.9920 - val_loss: 0.0225 - val_acc: 0.9930\n",
      "Epoch 47/50\n",
      " - 21s - loss: 0.0238 - acc: 0.9924 - val_loss: 0.0224 - val_acc: 0.9928\n",
      "Epoch 48/50\n",
      " - 21s - loss: 0.0236 - acc: 0.9923 - val_loss: 0.0223 - val_acc: 0.9930\n",
      "Epoch 49/50\n",
      " - 21s - loss: 0.0234 - acc: 0.9925 - val_loss: 0.0222 - val_acc: 0.9932\n",
      "Epoch 50/50\n",
      " - 21s - loss: 0.0236 - acc: 0.9923 - val_loss: 0.0220 - val_acc: 0.9929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x139e90a9710>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.989615Z",
     "start_time": "2019-07-21T03:27:51.341Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred_bool = (y_pred > 0.5)\n",
    "\n",
    "predictions = y_pred_bool.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.991344Z",
     "start_time": "2019-07-21T03:27:51.561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 1],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 1],\n",
       "       [1, 0, 0, ..., 0, 0, 1],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.992898Z",
     "start_time": "2019-07-21T03:27:51.761Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def print_evaluation_scores(y_test, predicted):\n",
    "    \n",
    "    print('Accuracy:', accuracy_score(y_test, predicted))\n",
    "    print('F1-score macro:', f1_score(y_test, predicted, average='macro'))\n",
    "    print('F1-score micro:', f1_score(y_test, predicted, average='micro'))\n",
    "    print('F1-score weighted:', f1_score(y_test, predicted, average='weighted'))\n",
    "    print('Hamming_loss:', hamming_loss(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T03:27:53.994119Z",
     "start_time": "2019-07-21T03:27:52.337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9583530525319451\n",
      "F1-score macro: 0.9215854760791898\n",
      "F1-score micro: 0.9796195652173914\n",
      "F1-score weighted: 0.9783413531559724\n",
      "Hamming_loss: 0.00709891150023663\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_scores(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
